
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Pydeep Recsys &#8212; drl-recsys</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Recsim Catalyst" href="T219174_Recsim_Catalyst.html" />
    <link rel="prev" title="Batch-Constrained Deep Q-Learning" href="T985223_Batch_Constrained_Deep_Q_Learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">drl-recsys</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="R984600_DRL_in_RecSys.html">
   Deep Reinforcement Learning in Recommendation Systems
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L268705_Offline_Reinforcement_Learning.html">
   Offline Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L732057_Markov_Decision_Process.html">
   Markov Decision Process
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  RL Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T726861_Introduction_to_Gym_toolkit.html">
   Introduction to Gym toolkit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T589782_Code_Driven_Introduction_to_Reinforcement_Learning.html">
   Code-Driven Introduction to Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T705437_CartPole_using_Cross_Entropy.html">
   CartPole using Cross-Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T163940_FrozenLake_using_Cross_Entropy.html">
   FrozenLake using Cross-Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471382_FrozenLake_using_Value_Iteration.html">
   FrozenLake using Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T587798_FrozenLake_using_Q_Learning.html">
   FrozenLake using Q-Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T752494_CartPole_using_REINFORCE_in_PyTorch.html">
   CartPole using REINFORCE in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T294930_Cartpole_in_PyTorch.html">
   Cartpole in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T859183_Q_Learning_on_Lunar_Lander_and_Frozen_Lake.html">
   Q-Learning on Lunar Lander and Frozen Lake
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T441700_REINFORCE.html">
   REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T079716_Importance_sampling.html">
   Importance Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T759314_Kullback_Leibler_Divergence.html">
   Kullback-Leibler Divergence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T035236_MDP_with_Dynamic_Programming_in_PyTorch.html">
   MDP with Dynamic Programming in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T365137_REINFORCE_in_PyTorch.html">
   REINFORCE in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T159137_MDP_Basics_with_Inventory_Control.html">
   MDP Basics with Inventory Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T046728_n_step_algorithms_and_eligibility_traces.html">
   n-step algorithms and eligibility traces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T635579_Q_Learning_vs_SARSA_and_Q_Learning_extensions.html">
   Q-Learning vs SARSA and Q-Learning extensions
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  RecSys Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T000348_Multi_armed_Bandit_for_Banner_Ad.html">
   Multi-armed Bandit for Banner Ad
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T119194_Contextual_RL_Product_Recommender.html">
   Contextual Recommender with Vowpal Wabbit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T373316_Top_K_Off_Policy_Correction_for_a_REINFORCE_Recommender_System.html">
   Top-K Off-Policy Correction for a REINFORCE Recommender System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T239645_Neural_Interactive_Collaborative_Filtering.html">
   Neural Interactive Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T985223_Batch_Constrained_Deep_Q_Learning.html">
   Batch-Constrained Deep Q-Learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Pydeep Recsys
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T219174_Recsim_Catalyst.html">
   Recsim Catalyst
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T079222_Solving_Multi_armed_Bandit_Problems.html">
   Solving Multi-armed Bandit Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T734685_Deep_Reinforcement_Learning_in_Large_Discrete_Action_Spaces.html">
   Deep Reinforcement Learning in Large Discrete Action Spaces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/T616640_Pydeep_Recsys.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/drl-recsys/main?urlpath=tree/docs/T616640_Pydeep_Recsys.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sparsh-ai/drl-recsys/blob/main/docs/T616640_Pydeep_Recsys.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#movielens-fairness-gym-environment">
   MovieLens Fairness Gym Environment
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-a-random-agent">
   Training a Random Agent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experience-replay">
   Experience Replay
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-statictics">
   Learning statictics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-networks">
   Neural Networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#agents">
   Agents
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#movielens-fairness-environment">
   Movielens Fairness Environment
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#manager">
   Manager
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#run">
   Run
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-agent">
     Random agent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reinforce-actor-critic-and-rainbow-agent">
     REINFORCE, Actor-Critic, and Rainbow agent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dqn-slate">
     DQN Slate
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plot-comparison">
   Plot comparison
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="pydeep-recsys">
<h1>Pydeep Recsys<a class="headerlink" href="#pydeep-recsys" title="Permalink to this headline">¶</a></h1>
<p><center><img src='_images/T616640_1.png'></center></p>
<p>The <code class="docutils literal notranslate"><span class="pre">ReinforcementLearning</span></code> abstract class defines what functions agents are expected to have. Namely, to predict the next action to be taken on a given state, the next k best actions (for slate environments), and to store a particular experience (i.e., a state, action, reward, done, next observed state) tuple.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">BaseNetwork</span></code> base class implements helper functions like saving and loading from a file, freezing specific parameters, running backward propagation of a loss, plotting the gradient graph, configuring the hardware device (i.e., CPU or GPU).</p>
<p><code class="docutils literal notranslate"><span class="pre">LearningStatistics</span></code> module helps collect different metrics that agents may output while training, providing ways to retrieve, plot, and aggregate them on many levels (i.e., model, episode, time-step, environment).</p>
<p><code class="docutils literal notranslate"><span class="pre">Manager</span></code> module coordinates the sending and receiving of actions and states. Managers help with training agents, hyperparameter search, executing episodes, and printing overviews of environments.</p>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!git clone https://github.com/luksfarris/pydeeprecsys.git pydeeprecsys
%cd pydeeprecsys
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># !curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python -</span>
<span class="c1"># !/root/.local/bin/poetry show</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># prepare submodules
!git submodule init
!git submodule update --remote
# install ml fairness using custom setup script
!python mlfairnessgym.setup.py install
# download movielens data
!python -m mlfairnessgym.environments.recommenders.download_movielens
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="movielens-fairness-gym-environment">
<h2>MovieLens Fairness Gym Environment<a class="headerlink" href="#movielens-fairness-gym-environment" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install recsim simplejson
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># we start by importing the environment module</span>
<span class="kn">from</span> <span class="nn">pydeeprecsys.movielens_fairness_env</span> <span class="kn">import</span> <span class="n">MovieLensFairness</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="c1"># then we can create a gym environment with a particular slate size</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;MovieLensFairness-v0&#39;</span><span class="p">,</span> <span class="n">slate_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>gym.wrappers.time_limit.TimeLimit
</pre></div>
</div>
</div>
</div>
<p>Now we can understand what are states, actions and rewards in this environment. Let’s start with state</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Box(0.0, 1.0, (25,), float32)
</pre></div>
</div>
</div>
</div>
<p>So we have 25 variables, in the (0,1) range:</p>
<ul class="simple">
<li><p>4 variables represent scaled (sex, age, occupation, zip code)</p></li>
<li><p>19 variables, that are one-hot encodings for the category of the latest reviewed movie. Categories include Action, Crime, Thriller, and so on</p></li>
<li><p>1 variable for the user’s 5 star rating.</p></li>
<li><p>1 variable for the movie’s violence score.</p></li>
</ul>
<p>Let’s see how they look like</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.        , 0.01785714, 0.1       , 0.60660607, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ])
</pre></div>
</div>
</div>
</div>
<p>As you can see, the first 4 variables are set. Under the hood, the environment sampled a user from the user pool, and prepared it. The remaining variables are 0 because no movie has been recommended yet. Let’s see how recommendations are made:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MultiDiscrete([3883 3883 3883])
</pre></div>
</div>
</div>
</div>
<p>Since we set our slate_size to 3, this means that at each step the agent must recommend 3 movies. Recommendations are made based on the (discrete) movie identifier, that’s why the action space is of type MultiDiscrete. 3883 is the amount of available movies. Let’s make a recommendation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">random_slate</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">random_slate</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 152, 1843, 2778])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">random_slate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>After receiving a recommendation, the user makes a choice, and reviews the movie. Thanks to Recsim and MLFairnessGym we can:</p>
<ul class="simple">
<li><p>affect the behavior of users after being exposed to movies, by encoding addiction/boredom dynamics into the user embedding</p></li>
<li><p>encode the violence score into the reward, so that recommending too many violent movies brings a negative reward</p></li>
</ul>
<p>Let’s see the new state:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">state</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.        , 0.01785714, 0.1       , 0.60660607, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 1.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.55230223, 0.        ])
</pre></div>
</div>
</div>
</div>
<p>We can also inspect the reward, which is in range (0,1):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">reward</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.43209501042800574
</pre></div>
</div>
</div>
</div>
<p>And we can check if the episode is done. Currently, episodes are finished when the simulated user has rated 50 movies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">done</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-a-random-agent">
<h2>Training a Random Agent<a class="headerlink" href="#training-a-random-agent" title="Permalink to this headline">¶</a></h2>
<p>The Manager class facilitates a lot of things like training, hyperparameter optimization, and so on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install highway_env
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydeeprecsys.rl.manager</span> <span class="kn">import</span> <span class="n">MovieLensFairnessManager</span>
<span class="kn">from</span> <span class="nn">pydeeprecsys.rl.agents.agent</span> <span class="kn">import</span> <span class="n">RandomAgent</span>
<span class="kn">from</span> <span class="nn">pydeeprecsys.rl.learning_statistics</span> <span class="kn">import</span> <span class="n">LearningStatistics</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">manager</span><span class="o">.</span><span class="n">print_overview</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Reward threshold: inf 
Reward signal range: (-inf, inf) 
Maximum episode steps: 50 
Action apace size: Discrete(3883)
Observation space size Box(0.0, 1.0, (25,), float32) 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">action_space</span><span class="o">=</span><span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
<span class="n">stats</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
<span class="n">manager</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">max_episodes</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">should_print</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training...
Episode 199 Mean Rewards 31.38 Last Reward 18.44		
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">plot_learning_stats</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T616640_Pydeep_Recsys_29_0.png" src="_images/T616640_Pydeep_Recsys_29_0.png" />
</div>
</div>
</div>
<div class="section" id="experience-replay">
<h2>Experience Replay<a class="headerlink" href="#experience-replay" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">RandomState</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span><span class="p">,</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Any</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Experience</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;Experience&quot;</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">,</span> <span class="s2">&quot;next_state&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">PriorityExperience</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;PriorityExperience&quot;</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;experience&quot;</span><span class="p">,</span> <span class="s2">&quot;priority&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExperienceReplayBufferParameters</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Parameters to configure an experience replay buffer. &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_experiences</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
        <span class="n">minimum_experiences_to_start_predicting</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">RandomState</span> <span class="o">=</span> <span class="n">RandomState</span><span class="p">(),</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">minimum_experiences_to_start_predicting</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The batch size mus the larger than the burn in&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_experiences</span> <span class="o">=</span> <span class="n">max_experiences</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minimum_experiences_to_start_predicting</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">minimum_experiences_to_start_predicting</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PERBufferParameters</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Parameters to configure the priorititization of experiences in a</span>
<span class="sd">    Prioritized-Experience Replay Buffer&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">beta_growth</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_growth</span> <span class="o">=</span> <span class="n">beta_growth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExperienceBuffer</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">ready_to_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">sample_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]:</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">next_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExperienceReplayBuffer</span><span class="p">(</span><span class="n">ExperienceBuffer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">ExperienceReplayBufferParameters</span><span class="p">(),</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minimum_experiences_to_start_predicting</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">parameters</span><span class="o">.</span><span class="n">minimum_experiences_to_start_predicting</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">parameters</span><span class="o">.</span><span class="n">random_state</span>
        <span class="c1"># create double ended queue to store the experiences</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">parameters</span><span class="o">.</span><span class="n">max_experiences</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">parameters</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">sample_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot; Samples a given number of experiences from the queue &quot;&quot;&quot;</span>
        <span class="c1"># samples the index of `batch_size` different experiences from the replay memory</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="c1"># get the experiences</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
        <span class="c1"># returns a flattened list of the samples</span>
        <span class="k">return</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">experiences</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">next_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Stores a new experience in the queue &quot;&quot;&quot;</span>
        <span class="n">experience</span> <span class="o">=</span> <span class="n">Experience</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
        <span class="c1"># append to the right (end) of the queue</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">ready_to_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns true only if we had enough experiences to start predicting</span>
<span class="sd">        (measured by the burn in)&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">minimum_experiences_to_start_predicting</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PrioritizedExperienceReplayBuffer</span><span class="p">(</span><span class="n">ExperienceReplayBuffer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">buffer_parameters</span><span class="o">=</span><span class="n">ExperienceReplayBufferParameters</span><span class="p">(),</span>
        <span class="n">per_parameters</span><span class="o">=</span><span class="n">PERBufferParameters</span><span class="p">(),</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">buffer_parameters</span><span class="p">)</span>
        <span class="c1"># beta controls the effect of the weights (how much to learn from each</span>
        <span class="c1"># experience in the batch)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">per_parameters</span><span class="o">.</span><span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_growth</span> <span class="o">=</span> <span class="n">per_parameters</span><span class="o">.</span><span class="n">beta_growth</span>
        <span class="c1"># alpha controls the effect of the priority (how much priority is affected</span>
        <span class="c1"># by the loss)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">per_parameters</span><span class="o">.</span><span class="n">alpha</span>
        <span class="c1"># epsilon guarantees no experience has priority zero</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">per_parameters</span><span class="o">.</span><span class="n">epsilon</span>

    <span class="k">def</span> <span class="nf">priorities</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; Gets the priority for each experience in the queue &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="p">[</span><span class="n">e</span><span class="o">.</span><span class="n">priority</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">next_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;We include a priority to the experience. if the queue is empty, priority is 1 (max),</span>
<span class="sd">        otherwise we check the maximum priority in the queue&quot;&quot;&quot;</span>
        <span class="n">priorities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">priorities</span><span class="p">()</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">priorities</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">priorities</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">1.0</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">priority</span><span class="p">):</span>
            <span class="n">experience</span> <span class="o">=</span> <span class="n">Experience</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">priority_experience</span> <span class="o">=</span> <span class="n">PriorityExperience</span><span class="p">(</span><span class="n">experience</span><span class="p">,</span> <span class="n">priority</span><span class="p">)</span>
            <span class="c1"># append to the right (end) of the queue</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">priority_experience</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_beta</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;We want to grow the beta value slowly and linearly, starting at a value</span>
<span class="sd">        close to zero, and stopping at 1.0. This is for the Importance Sampling&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_growth</span>

    <span class="k">def</span> <span class="nf">update_priorities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">],</span> <span class="n">errors_from_batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;We want the priority of elements to be the TD error of plus an epsilon</span>
<span class="sd">        constant. The epsilon constant makes sure that no experience ever gets a</span>
<span class="sd">        priority zero. This prioritization strategy gives more importance to</span>
<span class="sd">        elements that bring more learning to the network.&quot;&quot;&quot;</span>
        <span class="n">experience_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">object</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">experience_indexes</span><span class="p">)):</span>
            <span class="n">error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">errors_from_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">error</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">[</span><span class="n">experience_indexes</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">[</span>
                    <span class="n">experience_indexes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="p">]</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">priority</span><span class="o">=</span><span class="n">error</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;We sample experiences using their priorities as weights for sampling. The</span>
<span class="sd">        effect of the priorities is controlled by the alpha parameter. This is</span>
<span class="sd">        already an advantage but it can introduce bias in a network by always</span>
<span class="sd">        choosing the same type of experiences for training. In order to fight this, we</span>
<span class="sd">        compute the weight of the experience (this is called Importance Sampling,</span>
<span class="sd">        or IP). We want the weights to decrease over time, this is controlled by</span>
<span class="sd">        the beta parameter.&quot;&quot;&quot;</span>
        <span class="c1"># calculate probabilities (alpha)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">priorities</span><span class="p">()</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">probabilities</span> <span class="o">/</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="c1"># sample experiences</span>
        <span class="n">buffer_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">a</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">experience</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
        <span class="c1"># importance Sampling</span>
        <span class="c1"># w_i = (1/N * 1/P_i) ^ beta</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">/</span> <span class="n">buffer_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">p</span><span class="p">[</span><span class="n">samples</span><span class="p">]))</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">weights</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_beta</span><span class="p">()</span>
        <span class="c1"># return experiences with weights</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">experiences</span><span class="p">))</span> <span class="o">+</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">weights</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">samples</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="learning-statictics">
<h2>Learning statictics<a class="headerlink" href="#learning-statictics" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;paper&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LearningStatistics</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">env_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collected_metrics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_name</span> <span class="o">=</span> <span class="n">env_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timestep</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">append_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">metric_value</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collected_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="n">metric_name</span><span class="p">,</span>
                <span class="s2">&quot;measurement&quot;</span><span class="p">:</span> <span class="n">metric_value</span><span class="p">,</span>
                <span class="s2">&quot;timestep&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">timestep</span><span class="p">,</span>
                <span class="s2">&quot;episode&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">episode</span><span class="p">,</span>
                <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
                <span class="s2">&quot;env&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_name</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">]:</span>
        <span class="n">measurements</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">v</span><span class="p">[</span><span class="s2">&quot;measurement&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_metrics</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="s2">&quot;metric&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">metric_name</span> <span class="ow">and</span> <span class="n">v</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">model</span> <span class="ow">and</span> <span class="n">v</span><span class="p">[</span><span class="s2">&quot;env&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">env</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">measurements</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">measurements</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">moving_rewards</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(</span><span class="s2">&quot;moving_rewards&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">episode_rewards</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(</span><span class="s2">&quot;episode_rewards&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">epsilon_values</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(</span><span class="s2">&quot;epsilon_values&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">loss_values</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot_rewards</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_rewards</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">moving_rewards</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">plot_learning_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># generate subplots</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Agent learning metrics&quot;</span><span class="p">)</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">set_figheight</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">set_figwidth</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="c1"># add data to plots</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_rewards</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Reward Sum&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">moving_rewards</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Reward Moving Average&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_values</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Epsilon Values&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_values</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="neural-networks">
<h2>Neural Networks<a class="headerlink" href="#neural-networks" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install torchviz
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">save</span><span class="p">,</span> <span class="n">load</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">torchviz</span> <span class="kn">import</span> <span class="n">make_dot</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Softmax</span><span class="p">,</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">Tanh</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="n">multinomial</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="nb">sum</span> <span class="k">as</span> <span class="n">torch_sum</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">LongTensor</span><span class="p">,</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">gather</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span><span class="p">,</span> <span class="n">ravel</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">MSELoss</span><span class="p">,</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">functional</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sequential_architecture</span><span class="p">(</span><span class="n">layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; Fully connected layers, with bias, and ReLU activation&quot;&quot;&quot;</span>
    <span class="n">architecture</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">architecture</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">))</span>
        <span class="n">architecture</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ReLU</span><span class="p">())</span>
    <span class="n">architecture</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">architecture</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NoisyLayer</span><span class="p">(</span><span class="n">Linear</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.017</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">),</span> <span class="n">sigma</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;epsilon_weight&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sigma_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">out_features</span><span class="p">,),</span> <span class="n">sigma</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;epsilon_bias&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_features</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;At every forward operation, feeds the weights and biases with normally</span>
<span class="sd">        distributed random variables with mean zero and std deviation 1. This means</span>
<span class="sd">        the bias and the weights will have a noise of:</span>
<span class="sd">        sigma (constant) * epsilon (random in range(-1,1))&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_weight</span><span class="o">.</span><span class="n">normal_</span><span class="p">()</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_bias</span><span class="o">.</span><span class="n">normal_</span><span class="p">()</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_bias</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_weight</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">bias</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BaseNetwork</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_detect_device</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_auto_detect_device</span><span class="p">():</span>
        <span class="n">has_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">has_cuda</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Writes the model&#39;s parameters to the given path. &quot;&quot;&quot;</span>
        <span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Reads the model&#39;s parameters from the given path. &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">soft_parameter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_network</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">update_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;When using target networks, this method updates the parameters of the current network</span>
<span class="sd">        using the parameters of the given source network. The update_rate is a float in</span>
<span class="sd">        range (0,1) and controls how the update affects the target (self). update_rate=0</span>
<span class="sd">        means a full deep copy, and update_rate=1 means the target does not update</span>
<span class="sd">        at all. This parameter is usually called Tau. This method is usually called</span>
<span class="sd">        an exponential moving average update.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">source_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
            <span class="n">t</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">update_rate</span><span class="p">)</span> <span class="o">+</span> <span class="n">s</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">update_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">run_backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Requires an optimizer property. Runs backward on the given loss, and</span>
<span class="sd">        steps the optimizer.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">disable_learning</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_to_tensorboard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_example</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">):</span>
        <span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output/writer/</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">input_example</span><span class="p">)</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">add_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">make_dot</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span>
            <span class="n">params</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()),</span>
            <span class="n">show_attrs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">show_saved</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="s2">&quot;pdf&quot;</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output/graphs/</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PolicyEstimator</span><span class="p">(</span><span class="n">BaseNetwork</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Estimates the policy function: the probability of each action being the</span>
<span class="sd">    best decision in a particular state.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">hidden_layers</span> <span class="o">+</span> <span class="p">[</span><span class="n">output_size</span><span class="p">]</span>
        <span class="n">architecture</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">architecture</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
            <span class="n">architecture</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Tanh</span><span class="p">())</span>
        <span class="n">architecture</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">architecture</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">architecture</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">action_probabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_probabilities</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">multinomial</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">prediction</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">prediction</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">reward_baseline</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">):</span>
        <span class="n">state_tensor</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">action_tensor</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="sd">&quot;&quot;&quot; Update logic from the Policy Gradient theorem. &quot;&quot;&quot;</span>
        <span class="n">action_probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">)</span>
        <span class="n">action_distribution</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">action_probabilities</span><span class="p">)</span>
        <span class="n">selected_log_probabilities</span> <span class="o">=</span> <span class="n">action_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action_tensor</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch_sum</span><span class="p">(</span><span class="o">-</span><span class="n">selected_log_probabilities</span> <span class="o">*</span> <span class="n">reward_baseline</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DeepQNetwork</span><span class="p">(</span><span class="n">BaseNetwork</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implementation of a Deep Q Network with a Sequential arquitecture. Layers are</span>
<span class="sd">    supposed to be provided as a list of torch modules.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">architecture</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span>
        <span class="n">discount_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">statistics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LearningStatistics</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">architecture</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="n">discount_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">statistics</span> <span class="o">=</span> <span class="n">statistics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">best_action_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">tuple</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="n">ravel</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">])</span>
        <span class="n">state_tensor</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">)</span>
        <span class="n">best_action</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">best_action</span>

    <span class="k">def</span> <span class="nf">learn_from</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experiences</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_loss</span><span class="p">(</span><span class="n">experiences</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># store loss in statistics</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">statistics</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">statistics</span><span class="o">.</span><span class="n">append_metric</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">statistics</span><span class="o">.</span><span class="n">append_metric</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experiences</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">next_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">experiences</span><span class="p">]</span>
        <span class="n">state_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">next_state_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">reward_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">action_tensors</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">LongTensor</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">actions</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">done_tensors</span> <span class="o">=</span> <span class="n">BoolTensor</span><span class="p">(</span><span class="n">dones</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">actions_for_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">state_tensors</span><span class="p">)</span>
        <span class="n">q_vals</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span><span class="n">actions_for_states</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">action_tensors</span><span class="p">)</span>
        <span class="n">next_actions</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">best_action_for_state</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">next_states</span><span class="p">]</span>
        <span class="n">next_action_tensors</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">LongTensor</span><span class="p">(</span><span class="n">next_actions</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">q_vals_next</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">next_state_tensors</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">next_action_tensors</span><span class="p">)</span>
        <span class="n">q_vals_next</span><span class="p">[</span><span class="n">done_tensors</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">expected_q_vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="n">q_vals_next</span> <span class="o">+</span> <span class="n">reward_tensors</span>
        <span class="k">return</span> <span class="n">MSELoss</span><span class="p">()(</span><span class="n">q_vals</span><span class="p">,</span> <span class="n">expected_q_vals</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">FloatTensor</span><span class="p">,</span> <span class="n">LongTensor</span><span class="p">,</span> <span class="n">BoolTensor</span><span class="p">,</span> <span class="n">gather</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span><span class="p">,</span> <span class="n">ravel</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">functional</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span>

<span class="k">class</span> <span class="nc">DuelingDDQN</span><span class="p">(</span><span class="n">BaseNetwork</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Dueling DQN with Double DQN and Noisy Networks &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_input</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_output</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">hidden_layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">noise_sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.17</span><span class="p">,</span>
        <span class="n">discount_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">statistics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LearningStatistics</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">hidden_layers</span><span class="p">:</span>
            <span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="n">discount_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_network</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">statistics</span> <span class="o">=</span> <span class="n">statistics</span>

    <span class="k">def</span> <span class="nf">_build_network</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">n_input</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_output</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Builds the dueling network with noisy layers, the value</span>
<span class="sd">        subnet and the advantage subnet. TODO: add `.to_device()` to Modules&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_layers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>
        <span class="n">fc_1</span><span class="p">,</span> <span class="n">fc_2</span><span class="p">,</span> <span class="n">value_size</span><span class="p">,</span> <span class="n">advantage_size</span> <span class="o">=</span> <span class="n">hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">fc_1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_2</span> <span class="o">=</span> <span class="n">NoisyLayer</span><span class="p">(</span><span class="n">fc_1</span><span class="p">,</span> <span class="n">fc_2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">noise_sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_subnet</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
            <span class="n">NoisyLayer</span><span class="p">(</span><span class="n">fc_2</span><span class="p">,</span> <span class="n">value_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">noise_sigma</span><span class="p">),</span>
            <span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">value_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">advantage_subnet</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
            <span class="n">NoisyLayer</span><span class="p">(</span><span class="n">fc_2</span><span class="p">,</span> <span class="n">advantage_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">noise_sigma</span><span class="p">),</span>
            <span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">Linear</span><span class="p">(</span><span class="n">advantage_size</span><span class="p">,</span> <span class="n">n_output</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates the forward between the layers&quot;&quot;&quot;</span>
        <span class="n">layer_1_out</span> <span class="o">=</span> <span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">layer_2_out</span> <span class="o">=</span> <span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_2</span><span class="p">(</span><span class="n">layer_1_out</span><span class="p">))</span>
        <span class="n">value_of_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_subnet</span><span class="p">(</span><span class="n">layer_2_out</span><span class="p">)</span>
        <span class="n">advantage_of_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">advantage_subnet</span><span class="p">(</span><span class="n">layer_2_out</span><span class="p">)</span>
        <span class="c1"># This is the Dueling DQN part</span>
        <span class="c1"># Combines V and A to get Q: Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a&#39;))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">q_values</span> <span class="o">=</span> <span class="n">value_of_state</span> <span class="o">+</span> <span class="p">(</span>
                <span class="n">advantage_of_state</span> <span class="o">-</span> <span class="n">advantage_of_state</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q_values</span> <span class="o">=</span> <span class="n">value_of_state</span> <span class="o">+</span> <span class="p">(</span><span class="n">advantage_of_state</span> <span class="o">-</span> <span class="n">advantage_of_state</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">q_values</span>

    <span class="k">def</span> <span class="nf">get_q_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">tuple</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="n">ravel</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">])</span>
        <span class="n">state_tensor</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">top_k_actions_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_q_values</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">top_indices</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">top_indices</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>  <span class="c1"># TODO: cpu() ?</span>

    <span class="k">def</span> <span class="nf">learn_with</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">buffer</span><span class="p">:</span> <span class="n">PrioritizedExperienceReplayBuffer</span><span class="p">,</span> <span class="n">target_network</span><span class="p">:</span> <span class="n">Module</span>
    <span class="p">):</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="n">buffer</span><span class="o">.</span><span class="n">sample_batch</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">td_error</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_td_error_and_weigths</span><span class="p">(</span>
            <span class="n">experiences</span><span class="p">,</span> <span class="n">target_network</span>
        <span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">td_error</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># store loss in statistics</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">statistics</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">statistics</span><span class="o">.</span><span class="n">append_metric</span><span class="p">(</span>
                    <span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">statistics</span><span class="o">.</span><span class="n">append_metric</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
        <span class="c1"># update buffer priorities</span>
        <span class="n">errors_from_batch</span> <span class="o">=</span> <span class="n">td_error</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">buffer</span><span class="o">.</span><span class="n">update_priorities</span><span class="p">(</span><span class="n">experiences</span><span class="p">,</span> <span class="n">errors_from_batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_calculate_td_error_and_weigths</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">experiences</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">],</span> <span class="n">target_network</span><span class="p">:</span> <span class="n">Module</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">experiences</span>
        <span class="p">]</span>
        <span class="c1"># convert to tensors</span>
        <span class="n">state_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">next_state_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">reward_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">action_tensors</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">LongTensor</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">actions</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">done_tensors</span> <span class="o">=</span> <span class="n">BoolTensor</span><span class="p">(</span><span class="n">dones</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">weight_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># the following logic is the DDQN update</span>
        <span class="c1"># Then we get the predicted actions for the states that came next</span>
        <span class="c1"># (using the main network)</span>
        <span class="n">actions_for_next_states</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">top_k_actions_for_state</span><span class="p">(</span><span class="n">s</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">next_state_tensors</span>
        <span class="p">]</span>
        <span class="n">actions_for_next_states_tensor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">LongTensor</span><span class="p">(</span><span class="n">actions_for_next_states</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Then we use them to get the estimated Q Values for these next states/actions,</span>
        <span class="c1"># according to the target network. Remember that the target network is a copy</span>
        <span class="c1"># of this one taken some steps ago</span>
        <span class="n">next_q_values</span> <span class="o">=</span> <span class="n">target_network</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">next_state_tensors</span><span class="p">)</span>
        <span class="c1"># now we get the q values for the actions that were predicted for the next state</span>
        <span class="c1"># we call detach() so no gradient will be backpropagated along this variable</span>
        <span class="n">next_q_values_for_actions</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span>
            <span class="n">next_q_values</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">actions_for_next_states_tensor</span>
        <span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="c1"># zero value for done timesteps</span>
        <span class="n">next_q_values_for_actions</span><span class="p">[</span><span class="n">done_tensors</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># bellman equation</span>
        <span class="n">expected_q_values</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="n">next_q_values_for_actions</span> <span class="o">+</span> <span class="n">reward_tensors</span>
        <span class="p">)</span>
        <span class="c1"># Then get the Q-Values of the main network for the selected actions</span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">state_tensors</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">action_tensors</span><span class="p">)</span>
        <span class="c1"># And compare them (this is the time-difference or TD error)</span>
        <span class="n">td_error</span> <span class="o">=</span> <span class="n">q_values</span> <span class="o">-</span> <span class="n">expected_q_values</span>
        <span class="k">return</span> <span class="n">td_error</span><span class="p">,</span> <span class="n">weight_tensors</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">FloatTensor</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>


<span class="k">class</span> <span class="nc">QValueEstimator</span><span class="p">(</span><span class="n">BaseNetwork</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Estimates the Q-value (expected return) of each (state,action) pair &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">inputs</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">outputs</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">sequential_architecture</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">actions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">FloatTensor</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TwinnedQValueEstimator</span><span class="p">(</span><span class="n">BaseNetwork</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Estimates the Q-value (expected return) of each (state,action) pair,</span>
<span class="sd">    using 2 independent estimators, and predicting with the minimum estimated Q-value&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q1</span> <span class="o">=</span> <span class="n">QValueEstimator</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q2</span> <span class="o">=</span> <span class="n">QValueEstimator</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">actions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">q1</span><span class="p">,</span> <span class="n">q2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">actions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">q1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
        <span class="n">q2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">q1</span><span class="p">,</span> <span class="n">q2</span>

    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">rewards</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">dones</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">next_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">actor</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span>
        <span class="n">target</span><span class="p">:</span> <span class="s2">&quot;TwinnedQValueEstimator&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">curr_q1</span><span class="p">,</span> <span class="n">curr_q2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
        <span class="n">target_q</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">calculate_target_q</span><span class="p">(</span>
            <span class="n">states</span><span class="p">,</span>
            <span class="n">actions</span><span class="p">,</span>
            <span class="n">rewards</span><span class="p">,</span>
            <span class="n">next_states</span><span class="p">,</span>
            <span class="n">dones</span><span class="p">,</span>
            <span class="n">target_critic</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># TD errors for updating priority weights</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">curr_q1</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">-</span> <span class="n">target_q</span><span class="p">)</span>
        <span class="c1"># Critic loss is mean squared TD errors with priority weights.</span>
        <span class="n">q1_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">curr_q1</span> <span class="o">-</span> <span class="n">target_q</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span>
        <span class="n">q2_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">curr_q2</span> <span class="o">-</span> <span class="n">target_q</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q1</span><span class="o">.</span><span class="n">run_backpropagation</span><span class="p">(</span><span class="n">q1_loss</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q2</span><span class="o">.</span><span class="n">run_backpropagation</span><span class="p">(</span><span class="n">q2_loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">errors</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">MSELoss</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">FloatTensor</span>


<span class="k">class</span> <span class="nc">ValueEstimator</span><span class="p">(</span><span class="n">BaseNetwork</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Estimates the value function: the expected return of being in a</span>
<span class="sd">    particular state&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">sequential_architecture</span><span class="p">(</span>
            <span class="p">[</span><span class="n">input_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">hidden_layers</span> <span class="o">+</span> <span class="p">[</span><span class="n">output_size</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">state_tensor</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">return_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">expected_return</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">return_value</span><span class="p">]))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">predicted_return</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">predicted_return</span><span class="p">,</span> <span class="n">expected_return</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">LOG_STD_MAX</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">LOG_STD_MIN</span> <span class="o">=</span> <span class="o">-</span><span class="mi">20</span>
<span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">1e-6</span>


<span class="k">class</span> <span class="nc">GaussianActor</span><span class="p">(</span><span class="n">BaseNetwork</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">entropy_coefficient</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">discount_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">network_output</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># estimation of means and standard deviations</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">inputs</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">network_output</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">sequential_architecture</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="c1"># TODO: implement entropy learning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">entropy_coefficient</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">discount_factor</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">:</span> <span class="n">FloatTensor</span><span class="p">):</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">log_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">states</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">log_std</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">LOG_STD_MIN</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">LOG_STD_MAX</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">log_std</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">):</span>
        <span class="n">states_tensor</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># calculate Gaussian distribusion of (mean, std)</span>
        <span class="n">means</span><span class="p">,</span> <span class="n">log_stds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">states_tensor</span><span class="p">)</span>
        <span class="n">stds</span> <span class="o">=</span> <span class="n">log_stds</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
        <span class="n">normals</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">)</span>
        <span class="c1"># sample actions</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="n">normals</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
        <span class="c1"># calculate entropies</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">normals</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">actions</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">EPSILON</span><span class="p">)</span>
        <span class="n">entropies</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">actions</span><span class="p">,</span> <span class="n">entropies</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">rewards</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">dones</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">next_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">critic</span><span class="p">:</span> <span class="n">TwinnedQValueEstimator</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; Calculates the loss, backpropagates, and returns the entropy. &quot;&quot;&quot;</span>
        <span class="c1"># We re-sample actions to calculate expectations of Q.</span>
        <span class="n">sampled_action</span><span class="p">,</span> <span class="n">entropy</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
        <span class="c1"># expectations of Q with clipped double Q technique</span>
        <span class="n">q1</span><span class="p">,</span> <span class="n">q2</span> <span class="o">=</span> <span class="n">critic</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">sampled_action</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">q1</span><span class="p">,</span> <span class="n">q2</span><span class="p">)</span>
        <span class="c1"># Policy objective is maximization of (Q + alpha * entropy) with</span>
        <span class="c1"># priority weights.</span>
        <span class="n">actor_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="o">-</span><span class="n">q</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">entropy</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_backpropagation</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">entropy</span>

    <span class="k">def</span> <span class="nf">calculate_target_q</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">rewards</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">next_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">dones</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">target_critic</span><span class="p">:</span> <span class="n">TwinnedQValueEstimator</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># actor samples next actions</span>
            <span class="n">next_actions</span><span class="p">,</span> <span class="n">next_entropies</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span>
            <span class="c1"># cricic estimates q values for next actions</span>
            <span class="n">next_q_critic</span> <span class="o">=</span> <span class="n">target_critic</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_states</span><span class="p">,</span> <span class="n">next_actions</span><span class="p">)</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="n">next_q_critic</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">next_entropies</span>
        <span class="n">target_q</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q</span>
        <span class="n">target_q</span><span class="p">[</span><span class="n">dones</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">target_q</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="agents">
<h2>Agents<a class="headerlink" href="#agents" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">Space</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">RandomState</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ReinforcementLearning</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">action_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">top_k_actions_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">new_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">(</span><span class="n">ReinforcementLearning</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An agent that randomly samples actions, regardless of the</span>
<span class="sd">    environment&#39;s state.&quot;&quot;&quot;</span>

    <span class="n">action_space</span><span class="p">:</span> <span class="n">Space</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action_space</span><span class="p">:</span> <span class="n">Space</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">action_space</span>
        <span class="c1"># we seed the state so actions are reproducible</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">action_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">top_k_actions_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">new_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DecayingEpsilonGreedy</span><span class="p">(</span><span class="n">ReinforcementLearning</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">initial_exploration_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">decay_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">minimum_exploration_probability</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">RandomState</span> <span class="o">=</span> <span class="n">RandomState</span><span class="p">(),</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">initial_exploration_probability</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minimum_exploration_probability</span> <span class="o">=</span> <span class="n">minimum_exploration_probability</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span> <span class="o">=</span> <span class="n">decay_rate</span>

    <span class="k">def</span> <span class="nf">action_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;With probability epsilon, we explore by sampling one of the random available actions.</span>
<span class="sd">        Otherwise we exploit by chosing the action with the highest Q value.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">explore</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exploit</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">_decay</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Slowly decrease the exploration probability. &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">minimum_exploration_probability</span>
        <span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">explore</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; Randomly selects an action&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">exploit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; Selects the best action known for the given state &quot;&quot;&quot;</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">RandomState</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">arange</span>


<span class="k">class</span> <span class="nc">DQNAgent</span><span class="p">(</span><span class="n">DecayingEpsilonGreedy</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; TODO: This agent needs to be fixed&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_layers</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
        <span class="n">network_update_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">initial_exploration_probability</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">decay_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">minimum_exploration_probability</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">buffer_burn_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">discount_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">RandomState</span> <span class="o">=</span> <span class="n">RandomState</span><span class="p">(),</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">initial_exploration_probability</span><span class="p">,</span>
            <span class="n">decay_rate</span><span class="p">,</span>
            <span class="n">minimum_exploration_probability</span><span class="p">,</span>
            <span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">architecture</span> <span class="o">=</span> <span class="n">sequential_architecture</span><span class="p">(</span>
            <span class="p">[</span><span class="n">input_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">hidden_layers</span> <span class="o">+</span> <span class="p">[</span><span class="n">output_size</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">DeepQNetwork</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">architecture</span><span class="p">,</span> <span class="n">discount_factor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">ExperienceReplayBuffer</span><span class="p">(</span>
            <span class="n">ExperienceReplayBufferParameters</span><span class="p">(</span>
                <span class="n">max_experiences</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
                <span class="n">minimum_experiences_to_start_predicting</span><span class="o">=</span><span class="n">buffer_burn_in</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network_update_frequency</span> <span class="o">=</span> <span class="n">network_update_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_update_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">ready_to_predict</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">network_update_frequency</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">sample_batch</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">learn_from</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">action_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">state_flat</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">ready_to_predict</span><span class="p">():</span>
            <span class="n">action</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">action_for_state</span><span class="p">(</span><span class="n">state_flat</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">explore</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_update_network</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">top_k_actions_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="c1"># TODO:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">explore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">exploit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">best_action_for_state</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">new_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">done</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">ready_to_predict</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_decay</span><span class="p">()</span>
        <span class="n">state_flat</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">new_state_flat</span> <span class="o">=</span> <span class="n">new_state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">store_experience</span><span class="p">(</span><span class="n">state_flat</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">new_state_flat</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ReinforceAgent</span><span class="p">(</span><span class="n">ReinforcementLearning</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Policy estimator using a value estimator as a baseline.</span>
<span class="sd">    It&#39;s on-policy, for discrete action spaces, and episodic environments.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">state_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_layers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">discount_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>  <span class="c1"># a.k.a gamma</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">hidden_layers</span><span class="p">:</span>
            <span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">state_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_estimator</span> <span class="o">=</span> <span class="n">PolicyEstimator</span><span class="p">(</span>
            <span class="n">state_size</span><span class="p">,</span>
            <span class="n">hidden_layers</span><span class="p">,</span>
            <span class="n">n_actions</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="n">discount_factor</span>
        <span class="c1"># starts the buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_buffer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">ExperienceReplayBuffer</span><span class="p">(</span>
            <span class="n">ExperienceReplayBufferParameters</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">top_k_actions_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">action_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_k_actions_for_state</span><span class="p">(</span><span class="n">state</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">new_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="n">state_flat</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">new_state_flat</span> <span class="o">=</span> <span class="n">new_state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">store_experience</span><span class="p">(</span><span class="n">state_flat</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">new_state_flat</span><span class="p">)</span>
        <span class="c1"># FIXME: should learn after every episode, or after every N experiences?</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>  <span class="c1"># and self.buffer.ready_to_predict():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learn_from_experiences</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset_buffer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">discounted_rewards</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rewards</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;From a list of rewards obtained in an episode, we calculate</span>
<span class="sd">        the return minus the baseline. The baseline is the list of discounted</span>
<span class="sd">        rewards minus the mean, divided by the standard deviation.&quot;&quot;&quot;</span>
        <span class="n">discount_r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
        <span class="n">timesteps</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
        <span class="n">reward_sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
            <span class="n">reward_sum</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="n">reward_sum</span>
            <span class="n">discount_r</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward_sum</span>
        <span class="n">return_mean</span> <span class="o">=</span> <span class="n">discount_r</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">return_std</span> <span class="o">=</span> <span class="n">discount_r</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
        <span class="n">baseline</span> <span class="o">=</span> <span class="p">(</span><span class="n">discount_r</span> <span class="o">-</span> <span class="n">return_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">return_std</span>
        <span class="k">return</span> <span class="n">baseline</span>

    <span class="k">def</span> <span class="nf">learn_from_experiences</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">)</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">next_states</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">experiences</span><span class="p">)</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discounted_rewards</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
        <span class="n">advantages_tensor</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">advantages</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_estimator</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_estimator</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">advantages_tensor</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ActorCriticAgent</span><span class="p">(</span><span class="n">ReinforcementLearning</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Policy estimator using a value estimator as a baseline.</span>
<span class="sd">    It&#39;s on-policy, for discrete action spaces, and episodic environments.</span>
<span class="sd">    This implementation uses stochastic policies.</span>
<span class="sd">    TODO: could be a sub class of reinforce&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_actions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">state_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">discount_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">actor_hidden_layers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">critic_hidden_layers</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">actor_learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">critic_learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">actor_hidden_layers</span><span class="p">:</span>
            <span class="n">actor_hidden_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">state_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">critic_hidden_layers</span><span class="p">:</span>
            <span class="n">critic_hidden_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">state_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episode_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_estimator</span> <span class="o">=</span> <span class="n">ValueEstimator</span><span class="p">(</span>
            <span class="n">state_size</span><span class="p">,</span>
            <span class="n">critic_hidden_layers</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">critic_learning_rate</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_estimator</span> <span class="o">=</span> <span class="n">PolicyEstimator</span><span class="p">(</span>
            <span class="n">state_size</span><span class="p">,</span>
            <span class="n">actor_hidden_layers</span><span class="p">,</span>
            <span class="n">n_actions</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">actor_learning_rate</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="n">discount_factor</span>
        <span class="c1"># starts the buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_buffer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">ExperienceReplayBuffer</span><span class="p">(</span>
            <span class="n">ExperienceReplayBufferParameters</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">top_k_actions_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">action_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_k_actions_for_state</span><span class="p">(</span><span class="n">state</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">new_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="n">state_flat</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">new_state_flat</span> <span class="o">=</span> <span class="n">new_state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">store_experience</span><span class="p">(</span><span class="n">state_flat</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">new_state_flat</span><span class="p">)</span>
        <span class="c1"># FIXME: should learn after every episode, or after every N experiences?</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>  <span class="c1"># and self.buffer.ready_to_predict():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learn_from_experiences</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset_buffer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">learn_from_experiences</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">experience_queue</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">experience</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">experiences</span><span class="p">):</span>
            <span class="n">total_return</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">experiences</span><span class="p">[</span><span class="n">timestep</span><span class="p">:]):</span>
                <span class="n">total_return</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">**</span> <span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span><span class="o">.</span><span class="n">reward</span>

            <span class="c1"># Calculate baseline/advantage</span>
            <span class="n">baseline_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">experience</span><span class="o">.</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">advantage</span> <span class="o">=</span> <span class="n">total_return</span> <span class="o">-</span> <span class="n">baseline_value</span>
            <span class="c1"># Update our value estimator</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_estimator</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">experience</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="n">total_return</span><span class="p">)</span>
            <span class="c1"># Update our policy estimator</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy_estimator</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">experience</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="n">advantage</span><span class="p">,</span> <span class="n">experience</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>


<span class="k">class</span> <span class="nc">SoftActorCritic</span><span class="p">(</span><span class="n">ReinforcementLearning</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;TODO: there&#39;s things to fix in this agent. It needs temperature</span>
<span class="sd">    optimization, and replace the current q-value estimator with the</span>
<span class="sd">    Q-value + value + value_target estimators, like described here</span>
<span class="sd">    https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Space</span><span class="p">,</span>
        <span class="n">state_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">timesteps_to_start_predicting</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span>
        <span class="n">soft_target_update_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.005</span><span class="p">,</span>
        <span class="n">entropy_coefficient</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">target_update_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">discount_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">buffer_parameters</span><span class="o">=</span><span class="n">ExperienceReplayBufferParameters</span><span class="p">(),</span>
        <span class="n">per_parameters</span><span class="o">=</span><span class="n">PERBufferParameters</span><span class="p">(),</span>
    <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">action_space</span>
        <span class="n">n_actions</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># TODO: slate size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor</span> <span class="o">=</span> <span class="n">GaussianActor</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">state_size</span><span class="p">,</span>
            <span class="n">outputs</span><span class="o">=</span><span class="n">n_actions</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">entropy_coefficient</span><span class="o">=</span><span class="n">entropy_coefficient</span><span class="p">,</span>
            <span class="n">discount_factor</span><span class="o">=</span><span class="n">discount_factor</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic</span> <span class="o">=</span> <span class="n">TwinnedQValueEstimator</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">state_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_critic</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">PrioritizedExperienceReplayBuffer</span><span class="p">(</span>
            <span class="n">buffer_parameters</span><span class="o">=</span><span class="n">ExperienceReplayBufferParameters</span><span class="p">(),</span>
            <span class="n">per_parameters</span><span class="o">=</span><span class="n">PERBufferParameters</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="c1"># disable gradient calculations of the target network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_critic</span><span class="o">.</span><span class="n">disable_learning</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">timesteps_to_start_predicting</span> <span class="o">=</span> <span class="n">timesteps_to_start_predicting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timesteps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_steps</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># times the network was trained</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">soft_target_update_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_update_interval</span> <span class="o">=</span> <span class="n">target_update_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">discount_factor</span>

    <span class="k">def</span> <span class="nf">should_update_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">timesteps</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">timesteps_to_start_predicting</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">ready_to_predict</span><span class="p">()</span>  <span class="c1"># noqa</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">action_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">timesteps</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">timesteps_to_start_predicting</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">explore</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">top_k_actions_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="c1"># TODO:</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">new_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timesteps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">state_flat</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">new_state_flat</span> <span class="o">=</span> <span class="n">new_state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">store_experience</span><span class="p">(</span><span class="n">state_flat</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">new_state_flat</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_update_network</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">explore</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="c1"># act with gaussian randomness</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">action_array</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">n_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
        <span class="k">return</span> <span class="n">action_array</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_actions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">exploit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="c1"># act without randomness</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">action_array</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">n_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
        <span class="k">return</span> <span class="n">action_array</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_actions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_steps</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_steps</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_update_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># instead of updating the target network &quot;the hard way&quot;, we use a Tau</span>
            <span class="c1"># parameter as a weighting factor to update the weights as an</span>
            <span class="c1"># exponential moving average. This is analogous to the target net update</span>
            <span class="c1"># in the DQN algorithm.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target_critic</span><span class="o">.</span><span class="n">soft_parameter_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="p">,</span> <span class="n">update_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>

        <span class="c1"># batch with indices and priority weights</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">sample_batch</span><span class="p">()</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">batch</span>
        <span class="p">]</span>
        <span class="c1"># convert to tensors</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="o">.</span><span class="n">device</span>
        <span class="n">state_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">next_state_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">reward_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">action_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">done_tensors</span> <span class="o">=</span> <span class="n">BoolTensor</span><span class="p">(</span><span class="n">dones</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">weight_tensors</span> <span class="o">=</span> <span class="n">FloatTensor</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="n">errors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="o">.</span><span class="n">calculate_loss</span><span class="p">(</span>
            <span class="n">state_tensors</span><span class="p">,</span>
            <span class="n">action_tensors</span><span class="p">,</span>
            <span class="n">reward_tensors</span><span class="p">,</span>
            <span class="n">done_tensors</span><span class="p">,</span>
            <span class="n">next_state_tensors</span><span class="p">,</span>
            <span class="n">weight_tensors</span><span class="p">,</span>
            <span class="n">actor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="p">,</span>
            <span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">target_critic</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">calculate_loss</span><span class="p">(</span>
            <span class="n">state_tensors</span><span class="p">,</span>
            <span class="n">action_tensors</span><span class="p">,</span>
            <span class="n">reward_tensors</span><span class="p">,</span>
            <span class="n">done_tensors</span><span class="p">,</span>
            <span class="n">next_state_tensors</span><span class="p">,</span>
            <span class="n">weight_tensors</span><span class="p">,</span>
            <span class="n">critic</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># update priority weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">update_priorities</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">errors</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">arange</span>


<span class="k">class</span> <span class="nc">RainbowDQNAgent</span><span class="p">(</span><span class="n">ReinforcementLearning</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Instead of sampling randomly from the buffer we prioritize experiences with PER</span>
<span class="sd">    Instead of epsilon-greedy we use gaussian noisy layers for exploration</span>
<span class="sd">    Instead of the Q value we calculate Value and Advantage (Dueling DQN).</span>
<span class="sd">    This implementation does not include the Categorical DQN part (yet).&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">network_update_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">network_sync_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">priority_importance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
        <span class="n">priority_weigth_growth</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">buffer_burn_in</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">noise_sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.017</span><span class="p">,</span>
        <span class="n">discount_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.99</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span>
        <span class="n">hidden_layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="n">RandomState</span> <span class="o">=</span> <span class="n">RandomState</span><span class="p">(),</span>
        <span class="n">statistics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LearningStatistics</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">DuelingDDQN</span><span class="p">(</span>
            <span class="n">n_input</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
            <span class="n">n_output</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">noise_sigma</span><span class="o">=</span><span class="n">noise_sigma</span><span class="p">,</span>
            <span class="n">discount_factor</span><span class="o">=</span><span class="n">discount_factor</span><span class="p">,</span>
            <span class="n">statistics</span><span class="o">=</span><span class="n">statistics</span><span class="p">,</span>
            <span class="n">hidden_layers</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">PrioritizedExperienceReplayBuffer</span><span class="p">(</span>
            <span class="n">ExperienceReplayBufferParameters</span><span class="p">(</span>
                <span class="n">max_experiences</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">,</span>
                <span class="n">minimum_experiences_to_start_predicting</span><span class="o">=</span><span class="n">buffer_burn_in</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">PERBufferParameters</span><span class="p">(</span>
                <span class="n">alpha</span><span class="o">=</span><span class="n">priority_importance</span><span class="p">,</span>
                <span class="n">beta_growth</span><span class="o">=</span><span class="n">priority_weigth_growth</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network_update_frequency</span> <span class="o">=</span> <span class="n">network_update_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network_sync_frequency</span> <span class="o">=</span> <span class="n">network_sync_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

    <span class="k">def</span> <span class="nf">_check_update_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># we only start training the network once the buffer is ready</span>
        <span class="c1"># (the burn in is filled)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">ready_to_predict</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">network_update_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># we train at every K steps</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">learn_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">network_sync_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># at every N steps replaces the target network with the main network</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">top_k_actions_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="n">state_flat</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">ready_to_predict</span><span class="p">():</span>
            <span class="n">actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="o">.</span><span class="n">top_k_actions_for_state</span><span class="p">(</span><span class="n">state_flat</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_update_network</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">actions</span>

    <span class="k">def</span> <span class="nf">action_for_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_k_actions_for_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">store_experience</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">done</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">new_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">):</span>
        <span class="n">state_flat</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">new_state_flat</span> <span class="o">=</span> <span class="n">new_state</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">store_experience</span><span class="p">(</span><span class="n">state_flat</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">new_state_flat</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="movielens-fairness-environment">
<h2>Movielens Fairness Environment<a class="headerlink" href="#movielens-fairness-environment" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">from</span> <span class="nn">mlfairnessgym.environments.recommenders</span> <span class="kn">import</span> <span class="n">movie_lens_utils</span>
<span class="kn">from</span> <span class="nn">mlfairnessgym.environments.recommenders</span> <span class="kn">import</span> <span class="n">recsim_samplers</span>
<span class="kn">from</span> <span class="nn">mlfairnessgym.environments.recommenders</span> <span class="kn">import</span> <span class="n">movie_lens_dynamic</span> <span class="k">as</span> <span class="n">movie_lens</span>
<span class="kn">from</span> <span class="nn">recsim.simulator</span> <span class="kn">import</span> <span class="n">recsim_gym</span>
<span class="kn">from</span> <span class="nn">gym.envs.registration</span> <span class="kn">import</span> <span class="n">register</span>
<span class="kn">from</span> <span class="nn">gym.spaces</span> <span class="kn">import</span> <span class="n">Box</span><span class="p">,</span> <span class="n">Discrete</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">Env</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">_env_specs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;MovieLensFairness-v0&quot;</span><span class="p">,</span>
    <span class="s2">&quot;entry_point&quot;</span><span class="p">:</span> <span class="s2">&quot;pydeeprecsys.movielens_fairness_env:MovieLensFairness&quot;</span><span class="p">,</span>
    <span class="s2">&quot;max_episode_steps&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">register</span><span class="p">(</span><span class="o">**</span><span class="n">_env_specs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MovieLensFairness</span><span class="p">(</span><span class="n">Env</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; MovieLens + MLFairnessGym + Recsim + Gym environment &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span> <span class="o">=</span> <span class="n">slate_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_environment</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ndcg</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dcg</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">_get_product_relevance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">product_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; Relevance in range (0,1) &quot;&quot;&quot;</span>
        <span class="n">topic_affinity</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">user_model</span><span class="o">.</span><span class="n">_user_state</span><span class="o">.</span><span class="n">topic_affinity</span>
        <span class="p">)</span>
        <span class="n">movie_vector</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">d</span><span class="o">.</span><span class="n">movie_vec</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">_document_sampler</span><span class="o">.</span><span class="n">_corpus</span>
            <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">_doc_id</span> <span class="o">==</span> <span class="n">product_id</span>
        <span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">movie_vector</span><span class="p">,</span> <span class="n">topic_affinity</span><span class="p">),</span>
            <span class="n">movie_lens</span><span class="o">.</span><span class="n">User</span><span class="o">.</span><span class="n">MIN_SCORE</span><span class="p">,</span>
            <span class="n">movie_lens</span><span class="o">.</span><span class="n">User</span><span class="o">.</span><span class="n">MAX_SCORE</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_dcg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">relevances</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">relevances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">relevances</span><span class="p">))])</span>

    <span class="k">def</span> <span class="nf">_calculate_ndcg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slate_product_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">relevances</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_product_relevance</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">slate_product_ids</span><span class="p">]</span>
        <span class="n">dcg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dcg</span><span class="p">(</span><span class="n">relevances</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dcg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dcg</span><span class="p">)</span>
        <span class="n">ideal_relevances</span> <span class="o">=</span> <span class="p">[</span><span class="n">movie_lens</span><span class="o">.</span><span class="n">User</span><span class="o">.</span><span class="n">MAX_SCORE</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">relevances</span><span class="p">))]</span>
        <span class="n">idcg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dcg</span><span class="p">(</span><span class="n">ideal_relevances</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ndcg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dcg</span> <span class="o">/</span> <span class="n">idcg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]):</span>
        <span class="sd">&quot;&quot;&quot; Normalize reward and flattens/normalizes state &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_ndcg</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">encoded_state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">movielens_state_encoder</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">encoded_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">/</span> <span class="mi">5</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span><span class="o">.</span><span class="n">step</span><span class="p">([</span><span class="n">action</span><span class="p">])</span>
            <span class="n">encoded_state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">movielens_state_encoder</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="n">action</span><span class="p">],</span> <span class="n">info</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">encoded_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">/</span> <span class="mi">5</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; flattens/normalizes state &quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ndcg</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dcg</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">encoded_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">movielens_state_encoder</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[],</span> <span class="p">{})</span>
        <span class="k">return</span> <span class="n">encoded_state</span>

    <span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="n">close</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">action_space</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Discrete</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">nvec</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span><span class="o">.</span><span class="n">action_space</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reward_range</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">internal_env</span><span class="o">.</span><span class="n">reward_range</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">observation_space</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">movielens_state_encoder</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">action_slate</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">info</span><span class="p">:</span> <span class="nb">dict</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;if the slate size is &gt; 1, we need to guarantee the Single choice (SC)</span>
<span class="sd">        assumption, as described in the paper `SLATEQ: A Tractable Decomposition</span>
<span class="sd">        for Reinforcement Learning withRecommendation Sets`</span>
<span class="sd">        TODO: by randomly selecting one of the interactions?</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">user_features</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">]</span>
        <span class="n">response_features</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;response&quot;</span><span class="p">]</span>
        <span class="n">doc_features</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;doc&quot;</span><span class="p">][</span><span class="nb">str</span><span class="p">(</span><span class="n">action_slate</span><span class="p">[</span><span class="n">i</span><span class="p">])][</span><span class="s2">&quot;genres&quot;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">action_slate</span><span class="p">))</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">response_features</span><span class="p">:</span>
                <span class="n">chosen_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span><span class="p">)</span>
                <span class="n">response_features</span> <span class="o">=</span> <span class="p">(</span><span class="n">response_features</span><span class="p">[</span><span class="n">chosen_action</span><span class="p">],)</span>
                <span class="n">info</span><span class="p">[</span><span class="s2">&quot;chosen_action&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chosen_action</span>
            <span class="k">if</span> <span class="n">doc_features</span><span class="p">:</span>
                <span class="n">doc_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc_features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span><span class="p">)]]</span>

        <span class="n">refined_state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="n">user_features</span><span class="p">,</span>
            <span class="s2">&quot;response&quot;</span><span class="p">:</span> <span class="n">response_features</span><span class="p">,</span>
            <span class="s2">&quot;slate_docs&quot;</span><span class="p">:</span> <span class="n">doc_features</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="c1"># flattens the state</span>
        <span class="n">flat_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">refined_state</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">][</span><span class="s2">&quot;sex&quot;</span><span class="p">],</span>
                <span class="n">refined_state</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">][</span><span class="s2">&quot;age&quot;</span><span class="p">],</span>
                <span class="n">refined_state</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">][</span><span class="s2">&quot;occupation&quot;</span><span class="p">],</span>
                <span class="n">refined_state</span><span class="p">[</span><span class="s2">&quot;user&quot;</span><span class="p">][</span><span class="s2">&quot;zip_code&quot;</span><span class="p">],</span>
                <span class="o">*</span><span class="p">(</span>
                    <span class="n">refined_state</span><span class="p">[</span><span class="s2">&quot;slate_docs&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">refined_state</span><span class="p">[</span><span class="s2">&quot;slate_docs&quot;</span><span class="p">]</span>
                    <span class="k">else</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">19</span><span class="p">)</span>
                <span class="p">),</span>
                <span class="p">(</span><span class="n">refined_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;response&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">({},))[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;rating&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="p">(</span><span class="n">refined_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;responsse&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">({},))[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;violence_score&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">flat_state</span><span class="p">,</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">slate_action_selector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">qvals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Gets the index of the top N highest elements in the predictor array.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">qvals</span><span class="p">)[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span> <span class="p">:][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">prepare_environment</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">current_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>
        <span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">current_path</span><span class="p">,</span> <span class="s2">&quot;../output&quot;</span><span class="p">)</span>
        <span class="n">embeddings_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">current_path</span><span class="p">,</span>
            <span class="s2">&quot;../mlfairnessgym/environments/recommenders/movielens_factorization.json&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">env_config</span> <span class="o">=</span> <span class="n">movie_lens</span><span class="o">.</span><span class="n">EnvConfig</span><span class="p">(</span>
            <span class="n">seeds</span><span class="o">=</span><span class="n">movie_lens</span><span class="o">.</span><span class="n">Seeds</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">,</span>
            <span class="n">embeddings_path</span><span class="o">=</span><span class="n">embeddings_path</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">initial_embeddings</span> <span class="o">=</span> <span class="n">movie_lens_utils</span><span class="o">.</span><span class="n">load_embeddings</span><span class="p">(</span><span class="n">env_config</span><span class="p">)</span>
        <span class="c1"># user constructor</span>
        <span class="n">user_ctor</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
            <span class="n">movie_lens</span><span class="o">.</span><span class="n">User</span><span class="p">,</span> <span class="o">**</span><span class="n">attr</span><span class="o">.</span><span class="n">asdict</span><span class="p">(</span><span class="n">env_config</span><span class="o">.</span><span class="n">user_config</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">movie_lens_utils</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span>
            <span class="n">env_config</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span>
            <span class="n">user_ctor</span><span class="o">=</span><span class="n">user_ctor</span><span class="p">,</span>
            <span class="n">movie_ctor</span><span class="o">=</span><span class="n">movie_lens</span><span class="o">.</span><span class="n">Movie</span><span class="p">,</span>
            <span class="n">response_ctor</span><span class="o">=</span><span class="n">movie_lens</span><span class="o">.</span><span class="n">Response</span><span class="p">,</span>
            <span class="n">embeddings</span><span class="o">=</span><span class="n">initial_embeddings</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># the SingletonSampler will sample each movie once sequentially</span>
        <span class="n">document_sampler</span> <span class="o">=</span> <span class="n">recsim_samplers</span><span class="o">.</span><span class="n">SingletonSampler</span><span class="p">(</span>
            <span class="n">dataset</span><span class="o">.</span><span class="n">get_movies</span><span class="p">(),</span> <span class="n">movie_lens</span><span class="o">.</span><span class="n">Movie</span>
        <span class="p">)</span>
        <span class="n">user_sampler</span> <span class="o">=</span> <span class="n">recsim_samplers</span><span class="o">.</span><span class="n">UserPoolSampler</span><span class="p">(</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">env_config</span><span class="o">.</span><span class="n">seeds</span><span class="o">.</span><span class="n">user_sampler</span><span class="p">,</span>
            <span class="n">users</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">get_users</span><span class="p">(),</span>
            <span class="n">user_ctor</span><span class="o">=</span><span class="n">user_ctor</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">user_model</span> <span class="o">=</span> <span class="n">movie_lens</span><span class="o">.</span><span class="n">UserModel</span><span class="p">(</span>
            <span class="n">user_sampler</span><span class="o">=</span><span class="n">user_sampler</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">env_config</span><span class="o">.</span><span class="n">seeds</span><span class="o">.</span><span class="n">user_model</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">env</span> <span class="o">=</span> <span class="n">movie_lens</span><span class="o">.</span><span class="n">MovieLensEnvironment</span><span class="p">(</span>
            <span class="n">user_model</span><span class="p">,</span>
            <span class="n">document_sampler</span><span class="p">,</span>
            <span class="n">num_candidates</span><span class="o">=</span><span class="n">document_sampler</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
            <span class="n">slate_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span><span class="p">,</span>
            <span class="n">resample_documents</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">reward_aggregator</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
            <span class="n">movie_lens</span><span class="o">.</span><span class="n">multiobjective_reward</span><span class="p">,</span>
            <span class="n">lambda_non_violent</span><span class="o">=</span><span class="n">env_config</span><span class="o">.</span><span class="n">lambda_non_violent</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">recsim_env</span> <span class="o">=</span> <span class="n">recsim_gym</span><span class="o">.</span><span class="n">RecSimGymEnv</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">reward_aggregator</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">recsim_env</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="manager">
<h2>Manager<a class="headerlink" href="#manager" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">make</span><span class="p">,</span> <span class="n">spec</span><span class="p">,</span> <span class="n">Env</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span><span class="p">,</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">RandomState</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">highway_env</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># An episode output is a data model to represent 3 things: how many timesteps the</span>
<span class="c1"># episode took to finish, the total sum of rewards, and the average reward sum of the</span>
<span class="c1"># last 100 episodes.</span>
<span class="n">EpisodeOutput</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;EpisodeOutput&quot;</span><span class="p">,</span> <span class="s2">&quot;timesteps,reward_sum&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Manager</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Class for learning from gym environments with some convenience methods. &quot;&quot;&quot;</span>

    <span class="n">env_name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">Any</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">env_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Env</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_episode_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span>
        <span class="n">reward_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
            <span class="p">[</span><span class="n">env_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="n">env_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must specify exactly one of [env_name, env]&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">env_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_name</span> <span class="o">=</span> <span class="n">env_name</span>
            <span class="c1"># extract some parameters from the environment</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_episode_steps</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">spec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_name</span><span class="p">)</span><span class="o">.</span><span class="n">max_episode_steps</span> <span class="ow">or</span> <span class="n">max_episode_steps</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_threshold</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">spec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_name</span><span class="p">)</span><span class="o">.</span><span class="n">reward_threshold</span> <span class="ow">or</span> <span class="n">reward_threshold</span>
            <span class="p">)</span>
            <span class="c1"># create the environment</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="c1"># we seed the environment so that results are reproducible</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_episode_steps</span> <span class="o">=</span> <span class="n">max_episode_steps</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_threshold</span> <span class="o">=</span> <span class="n">reward_threshold</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">setup_reproducibility</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;slate_size&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;slate_size&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">print_overview</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Prints the most important variables of the environment. &quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reward threshold: </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_threshold</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reward signal range: </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reward_range</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximum episode steps: </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_episode_steps</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Action apace size: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Observation space size </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">execute_episodes</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rl</span><span class="p">:</span> <span class="n">ReinforcementLearning</span><span class="p">,</span>
        <span class="n">n_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">should_render</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">should_print</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">EpisodeOutput</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Execute any number of episodes with the given agent.</span>
<span class="sd">        Returns the number of timesteps and sum of rewards per episode.&quot;&quot;&quot;</span>
        <span class="n">episode_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
            <span class="n">t</span><span class="p">,</span> <span class="n">reward_sum</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">should_print</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">, starting at state </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span> <span class="ow">and</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_episode_steps</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">should_render</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">rl</span><span class="o">.</span><span class="n">action_for_state</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">should_print</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t=</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2"> a=</span><span class="si">{</span><span class="n">action</span><span class="si">}</span><span class="s2"> r=</span><span class="si">{</span><span class="n">reward</span><span class="si">}</span><span class="s2"> s=</span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>
                <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">episode_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EpisodeOutput</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">reward_sum</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">episode_outputs</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rl</span><span class="p">:</span> <span class="n">ReinforcementLearning</span><span class="p">,</span>
        <span class="n">statistics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LearningStatistics</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">should_print</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">should_print</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training...&quot;</span><span class="p">)</span>
        <span class="n">episode_rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_episodes</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">statistics</span><span class="p">:</span>
                <span class="n">statistics</span><span class="o">.</span><span class="n">episode</span> <span class="o">=</span> <span class="n">episode</span>
                <span class="n">statistics</span><span class="o">.</span><span class="n">timestep</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">while</span> <span class="n">done</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">action</span> <span class="o">=</span> <span class="n">rl</span><span class="o">.</span><span class="n">action_for_state</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">action</span> <span class="o">=</span> <span class="n">rl</span><span class="o">.</span><span class="n">top_k_actions_for_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">slate_size</span><span class="p">)</span>
                <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
                <span class="k">if</span> <span class="s2">&quot;chosen_action&quot;</span> <span class="ow">in</span> <span class="n">info</span><span class="p">:</span>
                    <span class="n">action</span> <span class="o">=</span> <span class="n">action</span><span class="p">[</span><span class="n">info</span><span class="p">[</span><span class="s2">&quot;chosen_action&quot;</span><span class="p">]]</span>
                <span class="n">rl</span><span class="o">.</span><span class="n">store_experience</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">new_state</span>
                <span class="p">)</span>  <span class="c1"># guardar experiencia en el buffer</span>
                <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">statistics</span><span class="p">:</span>
                    <span class="n">statistics</span><span class="o">.</span><span class="n">timestep</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">episode_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
            <span class="n">moving_average</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span>
            <span class="k">if</span> <span class="n">statistics</span><span class="p">:</span>
                <span class="n">statistics</span><span class="o">.</span><span class="n">append_metric</span><span class="p">(</span><span class="s2">&quot;episode_rewards&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
                <span class="n">statistics</span><span class="o">.</span><span class="n">append_metric</span><span class="p">(</span><span class="s2">&quot;timestep_rewards&quot;</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
                <span class="n">statistics</span><span class="o">.</span><span class="n">append_metric</span><span class="p">(</span><span class="s2">&quot;moving_rewards&quot;</span><span class="p">,</span> <span class="n">moving_average</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">should_print</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;</span><span class="se">\r</span><span class="s2">Episode </span><span class="si">{:d}</span><span class="s2"> Mean Rewards </span><span class="si">{:.2f}</span><span class="s2"> Last Reward </span><span class="si">{:.2f}</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">episode</span><span class="p">,</span> <span class="n">moving_average</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
                    <span class="p">),</span>
                    <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">moving_average</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_threshold</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">should_print</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reward threshold reached&quot;</span><span class="p">)</span>
                <span class="k">break</span>

    <span class="k">def</span> <span class="nf">hyperparameter_search</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">agent</span><span class="p">:</span> <span class="nb">type</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">default_params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="n">episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">runs_per_combination</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Given an agent class, and a dictionary of hyperparameter names and values,</span>
<span class="sd">        will try all combinations, and return the mean reward of each combinatio</span>
<span class="sd">        for the given number of episods, and will run the determined number of times.&quot;&quot;&quot;</span>
        <span class="n">combination_results</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">[])</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">p_name</span><span class="p">,</span> <span class="n">p_value</span><span class="p">)</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_value</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">p_value</span><span class="p">:</span>
                <span class="n">rl</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="o">**</span><span class="n">default_params</span><span class="p">,</span> <span class="n">p_name</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>
                <span class="n">learning_statistics</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
                <span class="n">combination_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">p_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">runs_per_combination</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                        <span class="n">rl</span><span class="o">=</span><span class="n">rl</span><span class="p">,</span>
                        <span class="n">max_episodes</span><span class="o">=</span><span class="n">episodes</span><span class="p">,</span>
                        <span class="n">should_print</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">statistics</span><span class="o">=</span><span class="n">learning_statistics</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">combination_results</span><span class="p">[</span><span class="n">combination_key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">learning_statistics</span><span class="o">.</span><span class="n">moving_rewards</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">Tested combination </span><span class="si">{</span><span class="n">p_name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2"> round </span><span class="si">{</span><span class="n">run</span><span class="si">}</span><span class="s2"> &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;result was </span><span class="si">{</span><span class="n">learning_statistics</span><span class="o">.</span><span class="n">moving_rewards</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
                            <span class="s2">&quot;</span><span class="se">\t\t</span><span class="s2">&quot;</span><span class="p">,</span>
                            <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
                        <span class="p">)</span>

        <span class="k">return</span> <span class="n">combination_results</span>

    <span class="k">def</span> <span class="nf">setup_reproducibility</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RandomState</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot; Seeds the project&#39;s libraries: numpy, torch, gym &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">seed</span><span class="p">:</span>
            <span class="c1"># seed pytorch</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="c1"># seed numpy</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
            <span class="c1"># seed gym</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span>


<span class="k">class</span> <span class="nc">HighwayManager</span><span class="p">(</span><span class="n">Manager</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">vehicles</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">env_name</span><span class="o">=</span><span class="s2">&quot;highway-v0&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">configure</span><span class="p">({</span><span class="s2">&quot;vehicles_count&quot;</span><span class="p">:</span> <span class="n">vehicles</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_episode_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;duration&quot;</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">CartpoleManager</span><span class="p">(</span><span class="n">Manager</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">env_name</span><span class="o">=</span><span class="s2">&quot;CartPole-v0&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_threshold</span> <span class="o">=</span> <span class="mi">50</span>


<span class="k">class</span> <span class="nc">LunarLanderManager</span><span class="p">(</span><span class="n">Manager</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">env_name</span><span class="o">=</span><span class="s2">&quot;LunarLander-v2&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MovieLensFairnessManager</span><span class="p">(</span><span class="n">Manager</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">slate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">env_name</span><span class="o">=</span><span class="s2">&quot;MovieLensFairness-v0&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">slate_size</span><span class="o">=</span><span class="n">slate_size</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="run">
<h2>Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h2>
<div class="section" id="random-agent">
<h3>Random agent<a class="headerlink" href="#random-agent" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">manager</span><span class="o">.</span><span class="n">print_overview</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Reward threshold: inf 
Reward signal range: (-inf, inf) 
Maximum episode steps: 50 
Action apace size: Discrete(3883)
Observation space size Box(0.0, 1.0, (25,), float32) 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">action_space</span><span class="o">=</span><span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
<span class="n">stats</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
<span class="n">manager</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">max_episodes</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">should_print</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training...
Episode 199 Mean Rewards 31.38 Last Reward 18.44		
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">plot_learning_stats</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T616640_Pydeep_Recsys_74_0.png" src="_images/T616640_Pydeep_Recsys_74_0.png" />
</div>
</div>
</div>
<div class="section" id="reinforce-actor-critic-and-rainbow-agent">
<h3>REINFORCE, Actor-Critic, and Rainbow agent<a class="headerlink" href="#reinforce-actor-critic-and-rainbow-agent" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_iterations</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">training_episodes</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_full_training</span><span class="p">(</span><span class="n">agent_class</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">trainings</span><span class="p">,</span> <span class="n">episodes</span><span class="p">):</span>
    <span class="n">statistics</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trainings</span><span class="p">):</span>
        <span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
        <span class="n">agent</span> <span class="o">=</span> <span class="n">agent_class</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>        
        <span class="n">manager</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">max_episodes</span><span class="o">=</span><span class="n">episodes</span><span class="p">,</span> <span class="n">statistics</span><span class="o">=</span><span class="n">stats</span><span class="p">)</span>
        <span class="n">statistics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">statistics</span><span class="p">)):</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">statistics</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">collected_metrics</span><span class="p">)</span>
        <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_class</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">metrics</span><span class="p">,</span> <span class="n">stats</span><span class="p">])</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">agent_class</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s1">_optimized_results.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">reinforce_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;n_actions&quot;</span><span class="p">:</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
    <span class="s2">&quot;state_size&quot;</span><span class="p">:</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;hidden_layers&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="s2">&quot;discount_factor&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">run_full_training</span><span class="p">(</span><span class="n">ReinforceAgent</span><span class="p">,</span> <span class="n">reinforce_params</span><span class="p">,</span> <span class="n">training_iterations</span><span class="p">,</span> <span class="n">training_episodes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training...
Episode 99 Mean Rewards 32.84 Last Reward 20.06		Training...
Episode 99 Mean Rewards 33.04 Last Reward 22.00		Training...
Episode 99 Mean Rewards 33.40 Last Reward 24.77		
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ac_params</span> <span class="o">=</span> <span class="n">default_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;n_actions&quot;</span><span class="p">:</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
    <span class="s2">&quot;state_size&quot;</span><span class="p">:</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;actor_hidden_layers&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="s2">&quot;critic_hidden_layers&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>
    <span class="s2">&quot;discount_factor&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
    <span class="s2">&quot;actor_learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="s2">&quot;critic_learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">run_full_training</span><span class="p">(</span><span class="n">ActorCriticAgent</span><span class="p">,</span> <span class="n">ac_params</span><span class="p">,</span> <span class="n">training_iterations</span><span class="p">,</span> <span class="n">training_episodes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training...
Episode 99 Mean Rewards 26.43 Last Reward 10.00		Training...
Episode 99 Mean Rewards 27.78 Last Reward 10.00		Training...
Episode 99 Mean Rewards 25.52 Last Reward 12.40		
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dqn_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;output_size&quot;</span><span class="p">:</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
    <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;network_update_frequency&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;network_sync_frequency&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s2">&quot;priority_importance&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
    <span class="s2">&quot;priority_weigth_growth&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="s2">&quot;buffer_burn_in&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">&quot;noise_sigma&quot;</span><span class="p">:</span> <span class="mf">0.017</span><span class="p">,</span>
    <span class="s2">&quot;discount_factor&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="s2">&quot;hidden_layers&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">run_full_training</span><span class="p">(</span><span class="n">RainbowDQNAgent</span><span class="p">,</span> <span class="n">dqn_params</span><span class="p">,</span> <span class="n">training_iterations</span><span class="p">,</span> <span class="n">training_episodes</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dqn-slate">
<h3>DQN Slate<a class="headerlink" href="#dqn-slate" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">dqn_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;output_size&quot;</span><span class="p">:</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">nvec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;network_update_frequency&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;network_sync_frequency&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s2">&quot;priority_importance&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
    <span class="s2">&quot;priority_weigth_growth&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>
    <span class="s2">&quot;buffer_burn_in&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">&quot;noise_sigma&quot;</span><span class="p">:</span> <span class="mf">0.017</span><span class="p">,</span>
    <span class="s2">&quot;discount_factor&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
    <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="s2">&quot;hidden_layers&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">seeds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">dcg_random_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ndcg_random_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dcg_dqn_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ndcg_dqn_results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seeds</span><span class="p">):</span>
    <span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
    <span class="n">manager</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">rl</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span><span class="n">statistics</span><span class="o">=</span><span class="n">stats</span><span class="p">,</span><span class="n">max_episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span><span class="n">should_print</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ndcg_random_results</span> <span class="o">+=</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">ndcg</span>
    <span class="n">dcg_random_results</span> <span class="o">+=</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">dcg</span>
    <span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">RainbowDQNAgent</span><span class="p">(</span><span class="o">**</span><span class="n">dqn_params</span><span class="p">)</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
    <span class="n">manager</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">rl</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span><span class="n">statistics</span><span class="o">=</span><span class="n">stats</span><span class="p">,</span><span class="n">max_episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span><span class="n">should_print</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ndcg_dqn_results</span> <span class="o">+=</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">ndcg</span>
    <span class="n">dcg_dqn_results</span> <span class="o">+=</span> <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">dcg</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;dcg_random&#39;</span><span class="p">:</span><span class="n">dcg_random_results</span><span class="p">,</span>
    <span class="s1">&#39;ndcg_random&#39;</span><span class="p">:</span><span class="n">ndcg_random_results</span><span class="p">,</span>
    <span class="s1">&#39;dcg_dqn&#39;</span><span class="p">:</span><span class="n">dcg_dqn_results</span><span class="p">,</span>
    <span class="s1">&#39;ndcg_dqn&#39;</span><span class="p">:</span><span class="n">ndcg_dqn_results</span><span class="p">,</span>
<span class="p">})</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;relevance_metrics.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;relevance_metrics_describe.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="plot-comparison">
<h2>Plot comparison<a class="headerlink" href="#plot-comparison" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_iterations</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">training_episodes</span> <span class="o">=</span> <span class="mi">300</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rainbow_statistics</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training_iterations</span><span class="p">):</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
    <span class="n">rainbow_agent</span> <span class="o">=</span> <span class="n">RainbowDQNAgent</span><span class="p">(</span>
        <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
        <span class="n">network_update_frequency</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">network_sync_frequency</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.00025</span><span class="p">,</span>
        <span class="n">discount_factor</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
        <span class="n">buffer_burn_in</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">statistics</span><span class="o">=</span><span class="n">stats</span>
    <span class="p">)</span>
    <span class="n">manager</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">rainbow_agent</span><span class="p">,</span> <span class="n">max_episodes</span><span class="o">=</span><span class="n">training_episodes</span><span class="p">,</span> <span class="n">statistics</span><span class="o">=</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">rainbow_statistics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>


<span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">random_statistics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training_iterations</span><span class="p">):</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
    <span class="n">random_agent</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="mi">3883</span><span class="p">))</span>
    <span class="n">manager</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">random_agent</span><span class="p">,</span> <span class="n">max_episodes</span><span class="o">=</span><span class="n">training_episodes</span><span class="p">,</span> <span class="n">statistics</span><span class="o">=</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">random_statistics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">reinforce_statistics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training_iterations</span><span class="p">):</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
    <span class="n">reinforce_agent</span> <span class="o">=</span> <span class="n">ReinforceAgent</span><span class="p">(</span>
        <span class="n">state_size</span><span class="o">=</span><span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
        <span class="n">n_actions</span><span class="o">=</span><span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> 
        <span class="n">discount_factor</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    <span class="n">manager</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">reinforce_agent</span><span class="p">,</span> <span class="n">max_episodes</span><span class="o">=</span><span class="n">training_episodes</span><span class="p">,</span> <span class="n">statistics</span><span class="o">=</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">reinforce_statistics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>


<span class="n">manager</span> <span class="o">=</span> <span class="n">MovieLensFairnessManager</span><span class="p">(</span><span class="n">slate_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ac_statistics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training_iterations</span><span class="p">):</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">LearningStatistics</span><span class="p">()</span>
    <span class="n">ac_agent</span> <span class="o">=</span> <span class="n">ActorCriticAgent</span><span class="p">(</span>
        <span class="n">state_size</span><span class="o">=</span><span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
        <span class="n">n_actions</span><span class="o">=</span><span class="n">manager</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> 
        <span class="n">discount_factor</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
    <span class="n">manager</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">ac_agent</span><span class="p">,</span> <span class="n">max_episodes</span><span class="o">=</span><span class="n">training_episodes</span><span class="p">,</span> <span class="n">statistics</span><span class="o">=</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">ac_statistics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training...
Episode 299 Mean Rewards 32.62 Last Reward 39.92		Training...
Episode 299 Mean Rewards 29.09 Last Reward 50.00		
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rainbow_statistics</span><span class="p">)):</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rainbow_statistics</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">collected_metrics</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;DuelingDQN&#39;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">metrics</span><span class="p">,</span> <span class="n">stats</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">random_statistics</span><span class="p">)):</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">random_statistics</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">collected_metrics</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;RandomAgent&#39;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">metrics</span><span class="p">,</span> <span class="n">stats</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">reinforce_statistics</span><span class="p">)):</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reinforce_statistics</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">collected_metrics</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;REINFORCE&#39;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">metrics</span><span class="p">,</span> <span class="n">stats</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ac_statistics</span><span class="p">)):</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ac_statistics</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">collected_metrics</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;ActorCritic&#39;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">metrics</span><span class="p">,</span> <span class="n">stats</span><span class="p">])</span>

<span class="n">metrics</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;all_metrics_test.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;all_metrics_test.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;metric==&quot;moving_rewards&quot;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;measurement&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">measurement</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">:(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">)})</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;paper&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;Set2&quot;</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;episode&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;measurement&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="mi">95</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Episodes&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Return Moving Average (100)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;preliminary_results.pdf&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T616640_Pydeep_Recsys_91_0.png" src="_images/T616640_Pydeep_Recsys_91_0.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sparsh-ai/drl-recsys",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T985223_Batch_Constrained_Deep_Q_Learning.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Batch-Constrained Deep Q-Learning</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T219174_Recsim_Catalyst.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Recsim Catalyst</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>