
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CartPole using Cross-Entropy &#8212; drl-recsys</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="FrozenLake using Cross-Entropy" href="T163940_FrozenLake_using_Cross_Entropy.html" />
    <link rel="prev" title="Code-Driven Introduction to Reinforcement Learning" href="T589782_Code_Driven_Introduction_to_Reinforcement_Learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">drl-recsys</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="R984600_DRL_in_RecSys.html">
   Deep Reinforcement Learning in Recommendation Systems
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L268705_Offline_Reinforcement_Learning.html">
   Offline Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L732057_Markov_Decision_Process.html">
   Markov Decision Process
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  RL Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T726861_Introduction_to_Gym_toolkit.html">
   Introduction to Gym toolkit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T589782_Code_Driven_Introduction_to_Reinforcement_Learning.html">
   Code-Driven Introduction to Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   CartPole using Cross-Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T163940_FrozenLake_using_Cross_Entropy.html">
   FrozenLake using Cross-Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471382_FrozenLake_using_Value_Iteration.html">
   FrozenLake using Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T587798_FrozenLake_using_Q_Learning.html">
   FrozenLake using Q-Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T752494_CartPole_using_REINFORCE_in_PyTorch.html">
   CartPole using REINFORCE in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T294930_Cartpole_in_PyTorch.html">
   Cartpole in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T859183_Q_Learning_on_Lunar_Lander_and_Frozen_Lake.html">
   Q-Learning on Lunar Lander and Frozen Lake
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T441700_REINFORCE.html">
   REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T079716_Importance_sampling.html">
   Importance Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T759314_Kullback_Leibler_Divergence.html">
   Kullback-Leibler Divergence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T035236_MDP_with_Dynamic_Programming_in_PyTorch.html">
   MDP with Dynamic Programming in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T365137_REINFORCE_in_PyTorch.html">
   REINFORCE in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T159137_MDP_Basics_with_Inventory_Control.html">
   MDP Basics with Inventory Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T046728_n_step_algorithms_and_eligibility_traces.html">
   n-step algorithms and eligibility traces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T635579_Q_Learning_vs_SARSA_and_Q_Learning_extensions.html">
   Q-Learning vs SARSA and Q-Learning extensions
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  RecSys Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T000348_Multi_armed_Bandit_for_Banner_Ad.html">
   Multi-armed Bandit for Banner Ad
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T119194_Contextual_RL_Product_Recommender.html">
   Contextual Recommender with Vowpal Wabbit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T373316_Top_K_Off_Policy_Correction_for_a_REINFORCE_Recommender_System.html">
   Top-K Off-Policy Correction for a REINFORCE Recommender System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T239645_Neural_Interactive_Collaborative_Filtering.html">
   Neural Interactive Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T985223_Batch_Constrained_Deep_Q_Learning.html">
   Batch-Constrained Deep Q-Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T616640_Pydeep_Recsys.html">
   Pydeep Recsys
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T219174_Recsim_Catalyst.html">
   Recsim Catalyst
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T079222_Solving_Multi_armed_Bandit_Problems.html">
   Solving Multi-armed Bandit Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T734685_Deep_Reinforcement_Learning_in_Large_Discrete_Action_Spaces.html">
   Deep Reinforcement Learning in Large Discrete Action Spaces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/T705437_CartPole_using_Cross_Entropy.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/drl-recsys/main?urlpath=tree/docs/T705437_CartPole_using_Cross_Entropy.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sparsh-ai/drl-recsys/blob/main/docs/T705437_CartPole_using_Cross_Entropy.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="cartpole-using-cross-entropy">
<h1>CartPole using Cross-Entropy<a class="headerlink" href="#cartpole-using-cross-entropy" title="Permalink to this headline">¶</a></h1>
<p>Despite the fact that it is much less famous than other tools in the RL practitioner’s toolbox, such as <strong>deep Q-network</strong> (<strong>DQN</strong>) or advantage actor-critic, the cross-entropy method has its own strengths. Firstly, the cross-entropy method is really simple, which makes it an easy method to follow. For example, its implementation on PyTorch is less than 100 lines of code.</p>
<p>Secondly, the method has good convergence. In simple environments that don’t require complex, multistep policies to be learned and discovered, and that have short episodes with frequent rewards, the cross-entropy method usually works very well. Of course, lots of practical problems don’t fall into this category, but sometimes they do. In such cases, the cross-entropy method (on its own or as part of a larger system) can be the perfect fit.</p>
<p>Cross-entropy method is model-free, policy-based, and on-policy, which means the following:</p>
<ul class="simple">
<li><p>It doesn’t build any model of the environment; it just says to the agent what to do at every step</p></li>
<li><p>It approximates the policy of the agent</p></li>
<li><p>It requires fresh data obtained from the environment</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
</pre></div>
</div>
</div>
</div>
<p>We define constants and they include the count of neurons in the hidden layer, the count of episodes we play on every iteration, and the percentile of episodes’ total rewards that we use for “elite” episode filtering. We will take the 70th percentile, which means that we will leave the top 30% of episodes sorted by reward.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">HIDDEN_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">PERCENTILE</span> <span class="o">=</span> <span class="mi">70</span>
</pre></div>
</div>
</div>
</div>
<p>Our model’s core is a one-hidden-layer NN, with rectified linear unit (ReLU) and 128 hidden neurons (which is absolutely arbitrary). Other hyperparameters are also set almost randomly and aren’t tuned, as the method is robust and converges very quickly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">obs_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There is nothing special about our NN; it takes a single observation from the environment as an input vector and outputs a number for every action we can perform. The output from the NN is a probability distribution over actions, so a straightforward way to proceed would be to include softmax nonlinearity after the last layer. However, in the preceding NN, we don’t apply softmax to increase the numerical stability of the training process. Rather than calculating softmax (which uses exponentiation) and then calculating cross-entropy loss (which uses a logarithm of probabilities), we can use the PyTorch class <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code>, which combines both softmax and cross-entropy in a single, more numerically stable expression. <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> requires raw, unnormalized values from the NN (also called logits). The downside of this is that we need to remember to apply softmax every time we need to get probabilities from our NN’s output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Episode</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Episode&#39;</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">,</span> <span class="s1">&#39;steps&#39;</span><span class="p">])</span>
<span class="n">EpisodeStep</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;EpisodeStep&#39;</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;observation&#39;</span><span class="p">,</span> <span class="s1">&#39;action&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Here we will define two helper classes that are named tuples from the <code class="docutils literal notranslate"><span class="pre">collections</span></code> package in the standard library:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">EpisodeStep</span></code>: This will be used to represent one single step that our agent made in the episode, and it stores the observation from the environment and what action the agent completed. We will use episode steps from “elite” episodes as training data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Episode</span></code>: This is a single episode stored as total undiscounted reward and a collection of <code class="docutils literal notranslate"><span class="pre">EpisodeStep</span></code>.</p></li>
</ul>
<p>Let’s look at a function that generates batches with episodes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">iterate_batches</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">episode_reward</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">episode_steps</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">sm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">obs_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">obs</span><span class="p">])</span>
        <span class="n">act_probs_v</span> <span class="o">=</span> <span class="n">sm</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">obs_v</span><span class="p">))</span>
        <span class="n">act_probs</span> <span class="o">=</span> <span class="n">act_probs_v</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">act_probs</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">act_probs</span><span class="p">)</span>
        <span class="n">next_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">is_done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">step</span> <span class="o">=</span> <span class="n">EpisodeStep</span><span class="p">(</span><span class="n">observation</span><span class="o">=</span><span class="n">obs</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">)</span>
        <span class="n">episode_steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_done</span><span class="p">:</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">Episode</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="n">episode_reward</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">episode_steps</span><span class="p">)</span>
            <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="n">episode_reward</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">episode_steps</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">next_obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">==</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">batch</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">next_obs</span>
</pre></div>
</div>
</div>
</div>
<p>The preceding function accepts the environment (the <code class="docutils literal notranslate"><span class="pre">Env</span></code> class instance from the Gym library), our NN, and the count of episodes it should generate on every iteration. The <code class="docutils literal notranslate"><span class="pre">batch</span></code> variable will be used to accumulate our batch (which is a list of <code class="docutils literal notranslate"><span class="pre">Episode</span></code> instances). We also declare a reward counter for the current episode and its list of steps (the <code class="docutils literal notranslate"><span class="pre">EpisodeStep</span></code> objects). Then we reset our environment to obtain the first observation and create a softmax layer, which will be used to convert the NN’s output to a probability distribution of actions. That’s our preparations complete, so we are ready to start the environment loop.</p>
<p>At every iteration, we convert our current observation to a PyTorch tensor and pass it to the NN to obtain action probabilities. There are several things to note here:</p>
<ul class="simple">
<li><p>All <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> instances in PyTorch expect a batch of data items and the same is true for our NN, so we convert our observation (which is a vector of four numbers in CartPole) into a tensor of size 1×4 (to achieve this, we pass an observation in a single-element list).</p></li>
<li><p>As we haven’t used nonlinearity at the output of our NN, it outputs raw action scores, which we need to feed through the softmax function.</p></li>
<li><p>Both our NN and the softmax layer return tensors that track gradients, so we need to unpack this by accessing the <code class="docutils literal notranslate"><span class="pre">tensor.data</span></code> field and then converting the tensor into a NumPy array. This array will have the same two-dimensional structure as the input, with the batch dimension on axis 0, so we need to get the first batch element to obtain a one-dimensional vector of action probabilities.</p></li>
</ul>
<p>Now that we have the probability distribution of actions, we can use it to obtain the actual action for the current step by sampling this distribution using NumPy’s function <code class="docutils literal notranslate"><span class="pre">random.choice()</span></code>. After this, we will pass this action to the environment to get our next observation, our reward, and the indication of the episode ending.</p>
<p>The reward is added to the current episode’s total reward, and our list of episode steps is also extended with an <code class="docutils literal notranslate"><span class="pre">(observation,</span> <span class="pre">action)</span></code> pair. Note that we save the observation that was used to choose the action, but not the observation returned by the environment as a result of the action. These are the tiny, but important, details that you need to keep in mind.</p>
<p>This is how we handle the situation when the current episode is over (in the case of CartPole, the episode ends when the stick has fallen down despite our efforts). We append the finalized episode to the batch, saving the total reward (as the episode has been completed and we have accumulated all the reward) and steps we have taken. Then we reset our total reward accumulator and clean the list of steps. After that, we reset our environment to start over.</p>
<p>In case our batch has reached the desired count of episodes, we return it to the caller for processing using <code class="docutils literal notranslate"><span class="pre">yield</span></code>. Our function is a generator, so every time the <code class="docutils literal notranslate"><span class="pre">yield</span></code> operator is executed, the control is transferred to the outer iteration loop and then continues after the <code class="docutils literal notranslate"><span class="pre">yield</span></code> line. If you are not familiar with Python’s generator functions, refer to the Python documentation: <a class="reference external" href="https://wiki.python.org/moin/Generators">https://wiki.python.org/moin/Generators</a>. After processing, we will clean up the batch.</p>
<p>The last, but very important, step in our loop is to assign an observation obtained from the environment to our current observation variable. After that, everything repeats infinitely—we pass the observation to the NN, sample the action to perform, ask the environment to process the action, and remember the result of this processing.</p>
<p>One very important fact to understand in this function logic is that the training of our NN and the generation of our episodes are performed <em>at the same time</em>. They are not completely in parallel, but every time our loop accumulates enough episodes (16), it passes control to this function caller, which is supposed to train the NN using gradient descent. So, when <code class="docutils literal notranslate"><span class="pre">yield</span></code> is returned, the NN will have different, slightly better (we hope) behavior.</p>
<p>We don’t need to explore proper synchronization, as our training and data gathering activities are performed at the same thread of execution, but you need to understand those constant jumps from NN training to its utilization.</p>
<p>Okay, now we need to define yet another function and then we will be ready to switch to the training loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">filter_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">percentile</span><span class="p">):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span> <span class="n">batch</span><span class="p">))</span>
    <span class="n">reward_bound</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">percentile</span><span class="p">)</span>
    <span class="n">reward_mean</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>

    <span class="n">train_obs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">train_act</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">reward</span><span class="p">,</span> <span class="n">steps</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">reward</span> <span class="o">&lt;</span> <span class="n">reward_bound</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">train_obs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="n">step</span><span class="o">.</span><span class="n">observation</span><span class="p">,</span> <span class="n">steps</span><span class="p">))</span>
        <span class="n">train_act</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="n">step</span><span class="o">.</span><span class="n">action</span><span class="p">,</span> <span class="n">steps</span><span class="p">))</span>

    <span class="n">train_obs_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">train_obs</span><span class="p">)</span>
    <span class="n">train_act_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">train_act</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_obs_v</span><span class="p">,</span> <span class="n">train_act_v</span><span class="p">,</span> <span class="n">reward_bound</span><span class="p">,</span> <span class="n">reward_mean</span>
</pre></div>
</div>
</div>
</div>
<p>This function is at the core of the cross-entropy method—from the given batch of episodes and percentile value, it calculates a boundary reward, which is used to filter “elite” episodes to train on. To obtain the boundary reward, we will use NumPy’s <code class="docutils literal notranslate"><span class="pre">percentile</span></code> function, which, from the list of values and the desired percentile, calculates the percentile’s value. Then, we will calculate the mean reward, which is used only for monitoring.</p>
<p>Next, we will filter off our episodes. For every episode in the batch, we will check that the episode has a higher total reward than our boundary and if it has, we will populate lists of observations and actions that we will train on.</p>
<p>As the final step of the function, we will convert our observations and actions from “elite” episodes into tensors, and return a tuple of four: observations, actions, the boundary of reward, and the mean reward. The last two values will be used only to write them into TensorBoard to check the performance of our agent.</p>
<p>Now, the final chunk of code that glues everything together, and mostly consists of the training loop, is as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v0&quot;</span><span class="p">)</span>
    <span class="c1"># env = gym.wrappers.Monitor(env, directory=&quot;mon&quot;, force=True)</span>
    <span class="n">obs_size</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">obs_size</span><span class="p">,</span> <span class="n">HIDDEN_SIZE</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
    <span class="n">objective</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">comment</span><span class="o">=</span><span class="s2">&quot;-cartpole&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">iter_no</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">iterate_batches</span><span class="p">(</span>
            <span class="n">env</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">BATCH_SIZE</span><span class="p">)):</span>
        <span class="n">obs_v</span><span class="p">,</span> <span class="n">acts_v</span><span class="p">,</span> <span class="n">reward_b</span><span class="p">,</span> <span class="n">reward_m</span> <span class="o">=</span> \
            <span class="n">filter_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">PERCENTILE</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">action_scores_v</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">obs_v</span><span class="p">)</span>
        <span class="n">loss_v</span> <span class="o">=</span> <span class="n">objective</span><span class="p">(</span><span class="n">action_scores_v</span><span class="p">,</span> <span class="n">acts_v</span><span class="p">)</span>
        <span class="n">loss_v</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2">: loss=</span><span class="si">%.3f</span><span class="s2">, reward_mean=</span><span class="si">%.1f</span><span class="s2">, rw_bound=</span><span class="si">%.1f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span>
            <span class="n">iter_no</span><span class="p">,</span> <span class="n">loss_v</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">reward_m</span><span class="p">,</span> <span class="n">reward_b</span><span class="p">))</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">loss_v</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">iter_no</span><span class="p">)</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;reward_bound&quot;</span><span class="p">,</span> <span class="n">reward_b</span><span class="p">,</span> <span class="n">iter_no</span><span class="p">)</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;reward_mean&quot;</span><span class="p">,</span> <span class="n">reward_m</span><span class="p">,</span> <span class="n">iter_no</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reward_m</span> <span class="o">&gt;</span> <span class="mi">199</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Solved!&quot;</span><span class="p">)</span>
            <span class="k">break</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0: loss=0.695, reward_mean=20.1, rw_bound=23.5
1: loss=0.681, reward_mean=32.9, rw_bound=36.5
2: loss=0.667, reward_mean=28.1, rw_bound=31.0
3: loss=0.660, reward_mean=33.0, rw_bound=36.5
4: loss=0.639, reward_mean=37.6, rw_bound=48.5
5: loss=0.641, reward_mean=48.1, rw_bound=49.5
6: loss=0.633, reward_mean=53.8, rw_bound=52.5
7: loss=0.609, reward_mean=59.6, rw_bound=66.0
8: loss=0.615, reward_mean=49.8, rw_bound=50.0
9: loss=0.600, reward_mean=65.1, rw_bound=78.0
10: loss=0.586, reward_mean=56.9, rw_bound=71.0
11: loss=0.599, reward_mean=68.3, rw_bound=77.0
12: loss=0.589, reward_mean=60.6, rw_bound=67.5
13: loss=0.586, reward_mean=61.2, rw_bound=72.5
14: loss=0.571, reward_mean=64.5, rw_bound=70.5
15: loss=0.562, reward_mean=68.9, rw_bound=84.0
16: loss=0.572, reward_mean=60.4, rw_bound=61.5
17: loss=0.563, reward_mean=87.5, rw_bound=91.0
18: loss=0.548, reward_mean=81.9, rw_bound=89.0
19: loss=0.575, reward_mean=91.8, rw_bound=99.0
20: loss=0.544, reward_mean=71.1, rw_bound=68.5
21: loss=0.528, reward_mean=78.4, rw_bound=85.5
22: loss=0.539, reward_mean=87.1, rw_bound=89.5
23: loss=0.517, reward_mean=83.2, rw_bound=91.5
24: loss=0.545, reward_mean=95.3, rw_bound=99.0
25: loss=0.532, reward_mean=94.1, rw_bound=103.5
26: loss=0.535, reward_mean=114.5, rw_bound=131.5
27: loss=0.530, reward_mean=112.2, rw_bound=134.0
28: loss=0.523, reward_mean=118.9, rw_bound=134.5
29: loss=0.540, reward_mean=133.2, rw_bound=154.0
30: loss=0.517, reward_mean=135.7, rw_bound=173.0
31: loss=0.537, reward_mean=124.8, rw_bound=154.5
32: loss=0.518, reward_mean=122.4, rw_bound=139.0
33: loss=0.515, reward_mean=170.8, rw_bound=193.5
34: loss=0.530, reward_mean=164.8, rw_bound=200.0
35: loss=0.515, reward_mean=176.1, rw_bound=200.0
36: loss=0.526, reward_mean=194.8, rw_bound=200.0
37: loss=0.514, reward_mean=185.1, rw_bound=200.0
38: loss=0.513, reward_mean=179.2, rw_bound=200.0
39: loss=0.526, reward_mean=195.6, rw_bound=200.0
40: loss=0.523, reward_mean=196.6, rw_bound=200.0
41: loss=0.520, reward_mean=195.2, rw_bound=200.0
42: loss=0.532, reward_mean=199.4, rw_bound=200.0
Solved!
</pre></div>
</div>
</div>
</div>
<p>In the training loop, we iterate our batches (a list of <code class="docutils literal notranslate"><span class="pre">Episode</span></code> objects), then we perform filtering of the “elite” episodes using the <code class="docutils literal notranslate"><span class="pre">filter_batch</span></code> function. The result is variables of observations and taken actions, the reward boundary used for filtering, and the mean reward. After that, we zero gradients of our NN and pass observations to the NN, obtaining its action scores. These scores are passed to the <code class="docutils literal notranslate"><span class="pre">objective</span></code> function, which will calculate cross-entropy between the NN output and the actions that the agent took. The idea of this is to reinforce our NN to carry out those “elite” actions that have led to good rewards. Then, we calculate gradients on the loss and ask the optimizer to adjust our NN.</p>
<p>The rest of the loop is mostly the monitoring of progress. On the console, we show the iteration number, the loss, the mean reward of the batch, and the reward boundary. We also write the same values to TensorBoard, to get a nice chart of the agent’s learning performance.</p>
<p>The last check in the loop is the comparison of the mean rewards of our batch episodes. When this becomes greater than 199, we stop our training. Why 199? In Gym, the CartPole environment is considered to be solved when the mean reward for the last 100 episodes is greater than 195, but our method converges so quickly that 100 episodes are usually what we need. The properly trained agent can balance the stick infinitely long (obtaining any amount of score), but the length of an episode in CartPole is limited to 200 steps (if you look at the environment variable of CartPole, you may notice the <code class="docutils literal notranslate"><span class="pre">TimeLimit</span></code> wrapper, which stops the episode after 200 steps). With all this in mind, we will stop training after the mean reward in the batch is greater than <code class="docutils literal notranslate"><span class="pre">199</span></code>, which is a good indication that our agent knows how to balance the stick like a pro.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">load_ext</span> <span class="n">tensorboard</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span> <span class="n">runs</span>
</pre></div>
</div>
</div>
</div>
<p><center><img src='_images/T705437_1.png'></center></p></div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sparsh-ai/drl-recsys",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T589782_Code_Driven_Introduction_to_Reinforcement_Learning.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Code-Driven Introduction to Reinforcement Learning</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T163940_FrozenLake_using_Cross_Entropy.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">FrozenLake using Cross-Entropy</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>