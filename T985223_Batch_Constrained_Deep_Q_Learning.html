
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Batch-Constrained Deep Q-Learning &#8212; drl-recsys</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Pydeep Recsys" href="T616640_Pydeep_Recsys.html" />
    <link rel="prev" title="Neural Interactive Collaborative Filtering" href="T239645_Neural_Interactive_Collaborative_Filtering.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">drl-recsys</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="R984600_DRL_in_RecSys.html">
   Deep Reinforcement Learning in Recommendation Systems
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L268705_Offline_Reinforcement_Learning.html">
   Offline Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L732057_Markov_Decision_Process.html">
   Markov Decision Process
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  RL Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T726861_Introduction_to_Gym_toolkit.html">
   Introduction to Gym toolkit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T589782_Code_Driven_Introduction_to_Reinforcement_Learning.html">
   Code-Driven Introduction to Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T705437_CartPole_using_Cross_Entropy.html">
   CartPole using Cross-Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T163940_FrozenLake_using_Cross_Entropy.html">
   FrozenLake using Cross-Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T471382_FrozenLake_using_Value_Iteration.html">
   FrozenLake using Value Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T587798_FrozenLake_using_Q_Learning.html">
   FrozenLake using Q-Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T752494_CartPole_using_REINFORCE_in_PyTorch.html">
   CartPole using REINFORCE in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T294930_Cartpole_in_PyTorch.html">
   Cartpole in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T859183_Q_Learning_on_Lunar_Lander_and_Frozen_Lake.html">
   Q-Learning on Lunar Lander and Frozen Lake
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T441700_REINFORCE.html">
   REINFORCE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T079716_Importance_sampling.html">
   Importance Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T759314_Kullback_Leibler_Divergence.html">
   Kullback-Leibler Divergence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T035236_MDP_with_Dynamic_Programming_in_PyTorch.html">
   MDP with Dynamic Programming in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T365137_REINFORCE_in_PyTorch.html">
   REINFORCE in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T159137_MDP_Basics_with_Inventory_Control.html">
   MDP Basics with Inventory Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T046728_n_step_algorithms_and_eligibility_traces.html">
   n-step algorithms and eligibility traces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T635579_Q_Learning_vs_SARSA_and_Q_Learning_extensions.html">
   Q-Learning vs SARSA and Q-Learning extensions
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  RecSys Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T000348_Multi_armed_Bandit_for_Banner_Ad.html">
   Multi-armed Bandit for Banner Ad
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T119194_Contextual_RL_Product_Recommender.html">
   Contextual Recommender with Vowpal Wabbit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T373316_Top_K_Off_Policy_Correction_for_a_REINFORCE_Recommender_System.html">
   Top-K Off-Policy Correction for a REINFORCE Recommender System
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T239645_Neural_Interactive_Collaborative_Filtering.html">
   Neural Interactive Collaborative Filtering
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Batch-Constrained Deep Q-Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T616640_Pydeep_Recsys.html">
   Pydeep Recsys
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T219174_Recsim_Catalyst.html">
   Recsim Catalyst
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T079222_Solving_Multi_armed_Bandit_Problems.html">
   Solving Multi-armed Bandit Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T734685_Deep_Reinforcement_Learning_in_Large_Discrete_Action_Spaces.html">
   Deep Reinforcement Learning in Large Discrete Action Spaces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T257798_Off_Policy_Learning_in_Two_stage_Recommender_Systems.html">
   Off-Policy Learning in Two-stage Recommender Systems
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/T985223_Batch_Constrained_Deep_Q_Learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/drl-recsys/main?urlpath=tree/docs/T985223_Batch_Constrained_Deep_Q_Learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sparsh-ai/drl-recsys/blob/main/docs/T985223_Batch_Constrained_Deep_Q_Learning.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#params">
   Params
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#replay-buffer">
   Replay buffer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#atari-preprocessing">
   Atari preprocessing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#create-environment">
   Create Environment
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dqn">
   DQN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generate-buffer">
   Generate Buffer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discrete-bcq">
   Discrete BCQ
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="batch-constrained-deep-q-learning">
<h1>Batch-Constrained Deep Q-Learning<a class="headerlink" href="#batch-constrained-deep-q-learning" title="Permalink to this headline">¶</a></h1>
<p>Current off-policy deep reinforcement learning algorithms fail to address extrapolation error by selecting actions with respect to a learned value estimate, without consideration of the accuracy of the estimate. As a result, certain outof-distribution actions can be erroneously extrapolated to higher values. However, the value of an off-policy agent can be accurately evaluated in regions where data is available.</p>
<p>Batch-Constrained deep Q-learning (BCQ), uses a state-conditioned generative model to produce only previously seen actions. This generative model is combined with a Q-network, to select the highest valued action which is similar to the data in the batch. Unlike any previous continuous control deep reinforcement learning algorithms, BCQ is able to learn successfully without interacting with the environment by considering extrapolation error.</p>
<p>BCQ is based on a simple idea: to avoid extrapolation error a policy should induce a similar state-action visitation to the batch. We denote policies which satisfy this notion as batch-constrained. To optimize off-policy learning for a given batch, batch-constrained policies are trained to select actions with respect to three objectives:</p>
<ol class="simple">
<li><p>Minimize the distance of selected actions to the data in the batch.</p></li>
<li><p>Lead to states where familiar data can be observed.</p></li>
<li><p>Maximize the value function.</p></li>
</ol>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install gym pyvirtualdisplay &gt; /dev/null 2&gt;&amp;1
!apt-get install -y xvfb python-opengl ffmpeg &gt; /dev/null 2&gt;&amp;1

!apt-get update &gt; /dev/null 2&gt;&amp;1
!apt-get install cmake &gt; /dev/null 2&gt;&amp;1
!pip install --upgrade setuptools 2&gt;&amp;1
!pip install ez_setup &gt; /dev/null 2&gt;&amp;1
!pip install gym[atari] &gt; /dev/null 2&gt;&amp;1

!wget http://www.atarimania.com/roms/Roms.rar
!mkdir /content/ROM/
!unrar e /content/Roms.rar /content/ROM/
!python -m atari_py.import_roms /content/ROM/
</pre></div>
</div>
</div>
</div>
<p>Restart the runtime. Required.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">gym.wrappers</span> <span class="kn">import</span> <span class="n">Monitor</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">pyvirtualdisplay</span> <span class="kn">import</span> <span class="n">Display</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span> <span class="k">as</span> <span class="n">ipythondisplay</span>

<span class="n">display</span> <span class="o">=</span> <span class="n">Display</span><span class="p">(</span><span class="n">visible</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1400</span><span class="p">,</span> <span class="mi">900</span><span class="p">))</span>
<span class="n">display</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Utility functions to enable video recording of gym environment </span>
<span class="sd">and displaying it.</span>
<span class="sd">To enable video, just do &quot;env = wrap_env(env)&quot;&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">show_video</span><span class="p">():</span>
  <span class="n">mp4list</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;video/*.mp4&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mp4list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">mp4</span> <span class="o">=</span> <span class="n">mp4list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">video</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">mp4</span><span class="p">,</span> <span class="s1">&#39;r+b&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
    <span class="n">ipythondisplay</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s1">&#39;&#39;&#39;&lt;video alt=&quot;test&quot; autoplay </span>
<span class="s1">                loop controls style=&quot;height: 400px;&quot;&gt;</span>
<span class="s1">                &lt;source src=&quot;data:video/mp4;base64,</span><span class="si">{0}</span><span class="s1">&quot; type=&quot;video/mp4&quot; /&gt;</span>
<span class="s1">             &lt;/video&gt;&#39;&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoded</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;ascii&#39;</span><span class="p">))))</span>
  <span class="k">else</span><span class="p">:</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Could not find video&quot;</span><span class="p">)</span>
    

<span class="k">def</span> <span class="nf">wrap_env</span><span class="p">(</span><span class="n">env</span><span class="p">):</span>
  <span class="n">env</span> <span class="o">=</span> <span class="n">Monitor</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s1">&#39;./video&#39;</span><span class="p">,</span> <span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">env</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">importlib</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="params">
<h2>Params<a class="headerlink" href="#params" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Args</span><span class="p">:</span>

    <span class="c1"># env = &quot;PongNoFrameskip-v0&quot; # OpenAI gym environment name</span>
    <span class="n">env</span> <span class="o">=</span> <span class="s2">&quot;CartPole-v0&quot;</span> <span class="c1"># OpenAI gym environment name</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Sets Gym, PyTorch and Numpy seeds</span>
    <span class="n">buffer_name</span> <span class="o">=</span> <span class="s2">&quot;Default&quot;</span> <span class="c1"># Prepends name to filename</span>
    <span class="n">max_timesteps</span> <span class="o">=</span> <span class="mf">1e4</span> <span class="c1"># Max time steps to run environment or train for</span>
    <span class="n">BCQ_threshold</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1"># Threshold hyper-parameter for BCQ</span>
    <span class="n">low_noise_p</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1"># Probability of a low noise episode when generating buffer</span>
    <span class="n">rand_action_p</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1"># Probability of taking a random action when generating buffer, during non-low noise episode</span>

    <span class="c1"># Atari Specific</span>
    <span class="n">atari_preprocessing</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;frame_skip&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="s2">&quot;frame_size&quot;</span><span class="p">:</span> <span class="mi">84</span><span class="p">,</span>
        <span class="s2">&quot;state_history&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="s2">&quot;done_on_life_loss&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;reward_clipping&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;max_episode_timesteps&quot;</span><span class="p">:</span> <span class="mf">27e3</span>
    <span class="p">}</span>
    
    <span class="n">atari_parameters</span> <span class="o">=</span> <span class="p">{</span>
		<span class="c1"># Exploration</span>
		<span class="s2">&quot;start_timesteps&quot;</span><span class="p">:</span> <span class="mf">2e4</span><span class="p">,</span>
		<span class="s2">&quot;initial_eps&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
		<span class="s2">&quot;end_eps&quot;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,</span>
		<span class="s2">&quot;eps_decay_period&quot;</span><span class="p">:</span> <span class="mf">25e4</span><span class="p">,</span>
		<span class="c1"># Evaluation</span>
		<span class="s2">&quot;eval_freq&quot;</span><span class="p">:</span> <span class="mf">5e4</span><span class="p">,</span>
		<span class="s2">&quot;eval_eps&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>
		<span class="c1"># Learning</span>
		<span class="s2">&quot;discount&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
		<span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="mf">1e6</span><span class="p">,</span>
		<span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
		<span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="s2">&quot;Adam&quot;</span><span class="p">,</span>
		<span class="s2">&quot;optimizer_parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
			<span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.0000625</span><span class="p">,</span>
			<span class="s2">&quot;eps&quot;</span><span class="p">:</span> <span class="mf">0.00015</span>
		<span class="p">},</span>
		<span class="s2">&quot;train_freq&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
		<span class="s2">&quot;polyak_target_update&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
		<span class="s2">&quot;target_update_freq&quot;</span><span class="p">:</span> <span class="mf">8e3</span><span class="p">,</span>
		<span class="s2">&quot;tau&quot;</span><span class="p">:</span> <span class="mi">1</span>
	<span class="p">}</span>
    
    <span class="n">regular_parameters</span> <span class="o">=</span> <span class="p">{</span>
		<span class="c1"># Exploration</span>
		<span class="s2">&quot;start_timesteps&quot;</span><span class="p">:</span> <span class="mf">1e3</span><span class="p">,</span>
		<span class="s2">&quot;initial_eps&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
		<span class="s2">&quot;end_eps&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
		<span class="s2">&quot;eps_decay_period&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
		<span class="c1"># Evaluation</span>
		<span class="s2">&quot;eval_freq&quot;</span><span class="p">:</span> <span class="mf">5e3</span><span class="p">,</span>
		<span class="s2">&quot;eval_eps&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
		<span class="c1"># Learning</span>
		<span class="s2">&quot;discount&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
		<span class="s2">&quot;buffer_size&quot;</span><span class="p">:</span> <span class="mf">1e6</span><span class="p">,</span>
		<span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
		<span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="s2">&quot;Adam&quot;</span><span class="p">,</span>
		<span class="s2">&quot;optimizer_parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
			<span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">3e-4</span>
		<span class="p">},</span>
		<span class="s2">&quot;train_freq&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
		<span class="s2">&quot;polyak_target_update&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
		<span class="s2">&quot;target_update_freq&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
		<span class="s2">&quot;tau&quot;</span><span class="p">:</span> <span class="mf">0.005</span>
	<span class="p">}</span>


<span class="n">args</span> <span class="o">=</span> <span class="n">Args</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;./results&quot;</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">&quot;./results&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;./models&quot;</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">&quot;./models&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;./buffers&quot;</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">&quot;./buffers&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set seeds</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="replay-buffer">
<h2>Replay buffer<a class="headerlink" href="#replay-buffer" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ReplayBuffer</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">is_atari</span><span class="p">,</span> <span class="n">atari_preprocessing</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
	<span class="k">if</span> <span class="n">is_atari</span><span class="p">:</span> 
		<span class="k">return</span> <span class="n">AtariBuffer</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">atari_preprocessing</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
	<span class="k">else</span><span class="p">:</span> 
		<span class="k">return</span> <span class="n">StandardBuffer</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
  

<span class="k">class</span> <span class="nc">AtariBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">atari_preprocessing</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">max_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">state_history</span> <span class="o">=</span> <span class="n">atari_preprocessing</span><span class="p">[</span><span class="s2">&quot;state_history&quot;</span><span class="p">]</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">=</span> <span class="mi">0</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">max_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
			<span class="n">atari_preprocessing</span><span class="p">[</span><span class="s2">&quot;frame_size&quot;</span><span class="p">],</span>
			<span class="n">atari_preprocessing</span><span class="p">[</span><span class="s2">&quot;frame_size&quot;</span><span class="p">]</span>
		<span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
		
		<span class="c1"># not_done only consider &quot;done&quot; if episode terminates due to failure condition</span>
		<span class="c1"># if episode terminates due to timelimit, the transition is not added to the buffer</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">not_done</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">first_timestep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">env_done</span><span class="p">,</span> <span class="n">first_timestep</span><span class="p">):</span>
		<span class="c1"># If dones don&#39;t match, env has reset due to timelimit</span>
		<span class="c1"># and we don&#39;t add the transition to the buffer</span>
		<span class="k">if</span> <span class="n">done</span> <span class="o">!=</span> <span class="n">env_done</span><span class="p">:</span>
			<span class="k">return</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">action</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">not_done</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">done</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">first_timestep</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">first_timestep</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

		<span class="c1"># Note + is concatenate here</span>
		<span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_history</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
		<span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

		<span class="n">state_not_done</span> <span class="o">=</span> <span class="mf">1.</span>
		<span class="n">next_not_done</span> <span class="o">=</span> <span class="mf">1.</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_history</span><span class="p">):</span>

			<span class="c1"># Wrap around if the buffer is filled</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">:</span>
				<span class="n">j</span> <span class="o">=</span> <span class="p">(</span><span class="n">ind</span> <span class="o">-</span> <span class="n">i</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span>
				<span class="n">k</span> <span class="o">=</span> <span class="p">(</span><span class="n">ind</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">j</span> <span class="o">=</span> <span class="n">ind</span> <span class="o">-</span> <span class="n">i</span>
				<span class="n">k</span> <span class="o">=</span> <span class="p">(</span><span class="n">ind</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
				<span class="c1"># If j == -1, then we set state_not_done to 0.</span>
				<span class="n">state_not_done</span> <span class="o">*=</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#np.where(j &lt; 0, state_not_done * 0, state_not_done)</span>
				<span class="n">j</span> <span class="o">=</span> <span class="n">j</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

			<span class="c1"># State should be all 0s if the episode terminated previously</span>
			<span class="n">state</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">state_not_done</span>
			<span class="n">next_state</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">next_not_done</span>

			<span class="c1"># If this was the first timestep, make everything previous = 0</span>
			<span class="n">next_not_done</span> <span class="o">*=</span> <span class="n">state_not_done</span>
			<span class="n">state_not_done</span> <span class="o">*=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_timestep</span><span class="p">[</span><span class="n">j</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

		<span class="k">return</span> <span class="p">(</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_done</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
		<span class="p">)</span>


	<span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_folder</span><span class="p">,</span> <span class="n">chunk</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">)):</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_action.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">])</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_reward.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">])</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_not_done.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">not_done</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">])</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_first_timestep.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_timestep</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">])</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_replay_info.npy&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">,</span> <span class="n">chunk</span><span class="p">])</span>

		<span class="n">crt</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
		<span class="k">while</span> <span class="n">crt</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_state_</span><span class="si">{</span><span class="n">end</span><span class="si">}</span><span class="s2">.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">crt</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
			<span class="n">crt</span> <span class="o">=</span> <span class="n">end</span>
			<span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">end</span> <span class="o">+</span> <span class="n">chunk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_folder</span><span class="p">,</span> <span class="n">size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
		<span class="n">reward_buffer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_reward.npy&quot;</span><span class="p">)</span>
		<span class="n">size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">)</span> <span class="k">if</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">reward_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="p">)</span>
		
		<span class="c1"># Adjust crt_size if we&#39;re using a custom size</span>
		<span class="n">size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">)</span> <span class="k">if</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">reward_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_action.npy&quot;</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward_buffer</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">not_done</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_not_done.npy&quot;</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">first_timestep</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_first_timestep.npy&quot;</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">,</span> <span class="n">chunk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_replay_info.npy&quot;</span><span class="p">)</span>

		<span class="n">crt</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
		<span class="k">while</span> <span class="n">crt</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">crt</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_state_</span><span class="si">{</span><span class="n">end</span><span class="si">}</span><span class="s2">.npy&quot;</span><span class="p">)</span>
			<span class="n">crt</span> <span class="o">=</span> <span class="n">end</span>
			<span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">end</span> <span class="o">+</span> <span class="n">chunk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>


<span class="c1"># Generic replay buffer for standard gym tasks</span>
<span class="k">class</span> <span class="nc">StandardBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">max_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">=</span> <span class="mi">0</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">not_done</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>


	<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">episode_done</span><span class="p">,</span> <span class="n">episode_start</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">action</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">next_state</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_state</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">not_done</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">done</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
		<span class="k">return</span> <span class="p">(</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">next_state</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
			<span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_done</span><span class="p">[</span><span class="n">ind</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
		<span class="p">)</span>


	<span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_folder</span><span class="p">):</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_state.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">])</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_action.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">])</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_next_state.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_state</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">])</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_reward.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">])</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_not_done.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">not_done</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">])</span>
		<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_ptr.npy&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_folder</span><span class="p">,</span> <span class="n">size</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
		<span class="n">reward_buffer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_reward.npy&quot;</span><span class="p">)</span>
		
		<span class="c1"># Adjust crt_size if we&#39;re using a custom size</span>
		<span class="n">size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">)</span> <span class="k">if</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">reward_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_state.npy&quot;</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_action.npy&quot;</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">next_state</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_next_state.npy&quot;</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward_buffer</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">not_done</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">save_folder</span><span class="si">}</span><span class="s2">_not_done.npy&quot;</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="p">]</span>

		<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Replay Buffer loaded with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">crt_size</span><span class="si">}</span><span class="s2"> elements.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="atari-preprocessing">
<h2>Atari preprocessing<a class="headerlink" href="#atari-preprocessing" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Atari Preprocessing</span>
<span class="c1"># Code is based on https://github.com/openai/gym/blob/master/gym/wrappers/atari_preprocessing.py</span>
<span class="k">class</span> <span class="nc">AtariPreprocessing</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span>
		<span class="n">env</span><span class="p">,</span>
		<span class="n">frame_skip</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
		<span class="n">frame_size</span><span class="o">=</span><span class="mi">84</span><span class="p">,</span>
		<span class="n">state_history</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
		<span class="n">done_on_life_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
		<span class="n">reward_clipping</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Clips to a range of -1,1</span>
		<span class="n">max_episode_timesteps</span><span class="o">=</span><span class="mi">27000</span>
	<span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">env</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">done_on_life_loss</span> <span class="o">=</span> <span class="n">done_on_life_loss</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span> <span class="o">=</span> <span class="n">frame_skip</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">frame_size</span> <span class="o">=</span> <span class="n">frame_size</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">reward_clipping</span> <span class="o">=</span> <span class="n">reward_clipping</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_steps</span> <span class="o">=</span> <span class="n">max_episode_timesteps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">frame_size</span><span class="p">,</span> <span class="n">frame_size</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">lives</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">episode_length</span> <span class="o">=</span> <span class="mi">0</span>

		<span class="c1"># Tracks previous 2 frames</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">frame_buffer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
			<span class="p">(</span><span class="mi">2</span><span class="p">,</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
			<span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span>
		<span class="p">)</span>
		<span class="c1"># Tracks previous 4 states</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">state_buffer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">state_history</span><span class="p">,</span> <span class="n">frame_size</span><span class="p">,</span> <span class="n">frame_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">lives</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">ale</span><span class="o">.</span><span class="n">lives</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">episode_length</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">ale</span><span class="o">.</span><span class="n">getScreenGrayscale</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frame_buffer</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">frame_buffer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">state_buffer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjust_frame</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">state_buffer</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_buffer</span>


	<span class="c1"># Takes single action is repeated for frame_skip frames (usually 4)</span>
	<span class="c1"># Reward is accumulated over those frames</span>
	<span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
		<span class="n">total_reward</span> <span class="o">=</span> <span class="mf">0.</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">episode_length</span> <span class="o">+=</span> <span class="mi">1</span>

		<span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span><span class="p">):</span>
			<span class="n">_</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
			<span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>

			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">done_on_life_loss</span><span class="p">:</span>
				<span class="n">crt_lives</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">ale</span><span class="o">.</span><span class="n">lives</span><span class="p">()</span>
				<span class="n">done</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">crt_lives</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">lives</span> <span class="k">else</span> <span class="n">done</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">lives</span> <span class="o">=</span> <span class="n">crt_lives</span>

			<span class="k">if</span> <span class="n">done</span><span class="p">:</span> 
				<span class="k">break</span>

			<span class="c1"># Second last and last frame</span>
			<span class="n">f</span> <span class="o">=</span> <span class="n">frame</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span> 
			<span class="k">if</span> <span class="n">f</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">ale</span><span class="o">.</span><span class="n">getScreenGrayscale</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frame_buffer</span><span class="p">[</span><span class="n">f</span><span class="p">])</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">state_buffer</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_buffer</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">state_buffer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adjust_frame</span><span class="p">()</span>

		<span class="n">done_float</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">done</span><span class="p">)</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">episode_length</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_steps</span><span class="p">:</span>
			<span class="n">done</span> <span class="o">=</span> <span class="kc">True</span>

		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_buffer</span><span class="p">,</span> <span class="n">total_reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">total_reward</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">done_float</span><span class="p">]</span>


	<span class="k">def</span> <span class="nf">adjust_frame</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="c1"># Take maximum over last two frames</span>
		<span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">frame_buffer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">frame_buffer</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
			<span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">frame_buffer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
		<span class="p">)</span>

		<span class="c1"># Resize</span>
		<span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">frame_buffer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
			<span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frame_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_size</span><span class="p">),</span>
			<span class="n">interpolation</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">INTER_AREA</span>
		<span class="p">)</span>
		<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="create-environment">
<h2>Create Environment<a class="headerlink" href="#create-environment" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create environment, add wrapper if necessary and create env_properties</span>
<span class="k">def</span> <span class="nf">make_env</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span> <span class="n">atari_preprocessing</span><span class="p">):</span>
	<span class="n">env</span> <span class="o">=</span> <span class="n">wrap_env</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">))</span>
	
	<span class="n">is_atari</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">envs</span><span class="o">.</span><span class="n">registry</span><span class="o">.</span><span class="n">spec</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span><span class="o">.</span><span class="n">entry_point</span> <span class="o">==</span> <span class="s1">&#39;gym.envs.atari:AtariEnv&#39;</span>
	<span class="n">env</span> <span class="o">=</span> <span class="n">AtariPreprocessing</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">atari_preprocessing</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_atari</span> <span class="k">else</span> <span class="n">env</span>

	<span class="n">state_dim</span> <span class="o">=</span> <span class="p">(</span>
		<span class="n">atari_preprocessing</span><span class="p">[</span><span class="s2">&quot;state_history&quot;</span><span class="p">],</span> 
		<span class="n">atari_preprocessing</span><span class="p">[</span><span class="s2">&quot;frame_size&quot;</span><span class="p">],</span> 
		<span class="n">atari_preprocessing</span><span class="p">[</span><span class="s2">&quot;frame_size&quot;</span><span class="p">]</span>
	<span class="p">)</span> <span class="k">if</span> <span class="n">is_atari</span> <span class="k">else</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

	<span class="k">return</span> <span class="p">(</span>
		<span class="n">env</span><span class="p">,</span>
		<span class="n">is_atari</span><span class="p">,</span>
		<span class="n">state_dim</span><span class="p">,</span>
		<span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
	<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dqn">
<h2>DQN<a class="headerlink" href="#dqn" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make env and determine properties</span>
<span class="n">env</span><span class="p">,</span> <span class="n">is_atari</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">num_actions</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_preprocessing</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_parameters</span> <span class="k">if</span> <span class="n">is_atari</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">regular_parameters</span>


<span class="c1"># Set seeds</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>


<span class="c1"># Initialize buffer</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">is_atari</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_preprocessing</span><span class="p">,</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">],</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;buffer_size&quot;</span><span class="p">],</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Used for Atari</span>
<span class="k">class</span> <span class="nc">Conv_Q</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frames</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">Conv_Q</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">c1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">c2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">c3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3136</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
		<span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
		<span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c2</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
		<span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c3</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
		<span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3136</span><span class="p">)))</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Used for Box2D / Toy problems</span>
<span class="k">class</span> <span class="nc">FC_Q</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">FC_Q</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">l3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
		<span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
		<span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">l3</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQN</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span> 
		<span class="n">is_atari</span><span class="p">,</span>
		<span class="n">num_actions</span><span class="p">,</span>
		<span class="n">state_dim</span><span class="p">,</span>
		<span class="n">device</span><span class="p">,</span>
		<span class="n">discount</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
		<span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span>
		<span class="n">optimizer_parameters</span><span class="o">=</span><span class="p">{},</span>
		<span class="n">polyak_target_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
		<span class="n">target_update_frequency</span><span class="o">=</span><span class="mf">8e3</span><span class="p">,</span>
		<span class="n">tau</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
		<span class="n">initial_eps</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
		<span class="n">end_eps</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
		<span class="n">eps_decay_period</span> <span class="o">=</span> <span class="mf">25e4</span><span class="p">,</span>
		<span class="n">eval_eps</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
	<span class="p">):</span>
	
		<span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

		<span class="c1"># Determine network type</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">Conv_Q</span><span class="p">(</span><span class="n">state_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_actions</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_atari</span> <span class="k">else</span> <span class="n">FC_Q</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_target</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_optimizer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_parameters</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">discount</span> <span class="o">=</span> <span class="n">discount</span>

		<span class="c1"># Target update rule</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">maybe_update_target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">polyak_target_update</span> <span class="k">if</span> <span class="n">polyak_target_update</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy_target_update</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">target_update_frequency</span> <span class="o">=</span> <span class="n">target_update_frequency</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>

		<span class="c1"># Decay for eps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">initial_eps</span> <span class="o">=</span> <span class="n">initial_eps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">end_eps</span> <span class="o">=</span> <span class="n">end_eps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">slope</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">end_eps</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_eps</span><span class="p">)</span> <span class="o">/</span> <span class="n">eps_decay_period</span>

		<span class="c1"># Evaluation hyper-parameters</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">state_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">state_dim</span> <span class="k">if</span> <span class="n">is_atari</span> <span class="k">else</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">eval_eps</span> <span class="o">=</span> <span class="n">eval_eps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>

		<span class="c1"># Number of training iterations</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">0</span>


	<span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
		<span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_eps</span> <span class="k">if</span> <span class="nb">eval</span> \
			<span class="k">else</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slope</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">iterations</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_eps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_eps</span><span class="p">)</span>

		<span class="c1"># Select action according to policy with probability (1-eps)</span>
		<span class="c1"># otherwise, select random action</span>
		<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span>
			<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
				<span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
				<span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replay_buffer</span><span class="p">):</span>
		<span class="c1"># Sample replay buffer</span>
		<span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

		<span class="c1"># Compute the target Q value</span>
		<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
			<span class="n">target_Q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">done</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_target</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

		<span class="c1"># Get current Q estimate</span>
		<span class="n">current_Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>

		<span class="c1"># Compute Q loss</span>
		<span class="n">Q_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="n">current_Q</span><span class="p">,</span> <span class="n">target_Q</span><span class="p">)</span>

		<span class="c1"># Optimize the Q</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
		<span class="n">Q_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

		<span class="c1"># Update target network by polyak or full copy every X iterations.</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">iterations</span> <span class="o">+=</span> <span class="mi">1</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">maybe_update_target</span><span class="p">()</span>


	<span class="k">def</span> <span class="nf">polyak_target_update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">target_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_target</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
		   <span class="n">target_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">copy_target_update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iterations</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_update_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
			 <span class="bp">self</span><span class="o">.</span><span class="n">Q_target</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>


	<span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
		<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">filename</span> <span class="o">+</span> <span class="s2">&quot;_Q&quot;</span><span class="p">)</span>
		<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">filename</span> <span class="o">+</span> <span class="s2">&quot;_optimizer&quot;</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filename</span> <span class="o">+</span> <span class="s2">&quot;_Q&quot;</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_target</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filename</span> <span class="o">+</span> <span class="s2">&quot;_optimizer&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Runs policy for X episodes and returns average reward</span>
<span class="c1"># A fixed seed is used for the eval environment</span>
<span class="k">def</span> <span class="nf">eval_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">env_name</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">eval_episodes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
	<span class="n">eval_env</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_preprocessing</span><span class="p">)</span>
	<span class="n">eval_env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">+</span> <span class="mi">100</span><span class="p">)</span>

	<span class="n">avg_reward</span> <span class="o">=</span> <span class="mf">0.</span>
	<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">eval_episodes</span><span class="p">):</span>
		<span class="n">state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">eval_env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="kc">False</span>
		<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
			<span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">state</span><span class="p">),</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
			<span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">eval_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
			<span class="n">avg_reward</span> <span class="o">+=</span> <span class="n">reward</span>

	<span class="n">avg_reward</span> <span class="o">/=</span> <span class="n">eval_episodes</span>

	<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---------------------------------------&quot;</span><span class="p">)</span>
	<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation over </span><span class="si">{</span><span class="n">eval_episodes</span><span class="si">}</span><span class="s2"> episodes: </span><span class="si">{</span><span class="n">avg_reward</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
	<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---------------------------------------&quot;</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">avg_reward</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># For saving files</span>
<span class="n">setting</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">buffer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">buffer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">setting</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Initialize and load policy</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span>
    <span class="n">is_atari</span><span class="p">,</span>
    <span class="n">num_actions</span><span class="p">,</span>
    <span class="n">state_dim</span><span class="p">,</span>
    <span class="n">device</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;discount&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;optimizer&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;optimizer_parameters&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;polyak_target_update&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;target_update_freq&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;tau&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;initial_eps&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;end_eps&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eps_decay_period&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eval_eps&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">evaluations</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="kc">False</span>
<span class="n">episode_start</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">episode_timesteps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">episode_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">low_noise_ep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">args</span><span class="o">.</span><span class="n">low_noise_p</span>
<span class="n">max_episode_steps</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="p">)</span><span class="o">.</span><span class="n">_max_episode_steps</span>

<span class="c1"># Interact with the environment for max_timesteps</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_timesteps</span><span class="p">)):</span>

    <span class="n">episode_timesteps</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;start_timesteps&quot;</span><span class="p">]:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>

    <span class="c1"># Perform action and log results</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>

    <span class="c1"># Only consider &quot;done&quot; if episode terminates due to failure condition</span>
    <span class="n">done_float</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">done</span><span class="p">)</span> <span class="k">if</span> <span class="n">episode_timesteps</span> <span class="o">&lt;</span> <span class="n">max_episode_steps</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># For atari, info[0] = clipped reward, info[1] = done_float</span>
    <span class="k">if</span> <span class="n">is_atari</span><span class="p">:</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">done_float</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
    <span class="c1"># Store data in replay buffer</span>
    <span class="n">replay_buffer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done_float</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">episode_start</span><span class="p">)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
    <span class="n">episode_start</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Train agent after collecting sufficient data</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;start_timesteps&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;train_freq&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="c1"># +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total T: </span><span class="si">{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> Episode Num: </span><span class="si">{</span><span class="n">episode_num</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> Episode T: </span><span class="si">{</span><span class="n">episode_timesteps</span><span class="si">}</span><span class="s2"> Reward: </span><span class="si">{</span><span class="n">episode_reward</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># Reset environment</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="kc">False</span>
        <span class="n">episode_start</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">episode_timesteps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">episode_num</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">low_noise_ep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">args</span><span class="o">.</span><span class="n">low_noise_p</span>

    <span class="c1"># Evaluate episode</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eval_freq&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">evaluations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eval_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">))</span>
        <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./results/behavioral_</span><span class="si">{</span><span class="n">setting</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">evaluations</span><span class="p">)</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./models/behavioral_</span><span class="si">{</span><span class="n">setting</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Save final policy</span>
<span class="n">policy</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./models/behavioral_</span><span class="si">{</span><span class="n">setting</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total T: 52 Episode Num: 1 Episode T: 52 Reward: 52.000
Total T: 107 Episode Num: 2 Episode T: 55 Reward: 55.000
Total T: 136 Episode Num: 3 Episode T: 29 Reward: 29.000
Total T: 156 Episode Num: 4 Episode T: 20 Reward: 20.000
Total T: 168 Episode Num: 5 Episode T: 12 Reward: 12.000
Total T: 187 Episode Num: 6 Episode T: 19 Reward: 19.000
Total T: 207 Episode Num: 7 Episode T: 20 Reward: 20.000
Total T: 220 Episode Num: 8 Episode T: 13 Reward: 13.000
Total T: 249 Episode Num: 9 Episode T: 29 Reward: 29.000
Total T: 268 Episode Num: 10 Episode T: 19 Reward: 19.000
Total T: 286 Episode Num: 11 Episode T: 18 Reward: 18.000
Total T: 306 Episode Num: 12 Episode T: 20 Reward: 20.000
Total T: 321 Episode Num: 13 Episode T: 15 Reward: 15.000
Total T: 368 Episode Num: 14 Episode T: 47 Reward: 47.000
Total T: 405 Episode Num: 15 Episode T: 37 Reward: 37.000
Total T: 429 Episode Num: 16 Episode T: 24 Reward: 24.000
Total T: 461 Episode Num: 17 Episode T: 32 Reward: 32.000
Total T: 475 Episode Num: 18 Episode T: 14 Reward: 14.000
Total T: 500 Episode Num: 19 Episode T: 25 Reward: 25.000
Total T: 519 Episode Num: 20 Episode T: 19 Reward: 19.000
Total T: 528 Episode Num: 21 Episode T: 9 Reward: 9.000
Total T: 546 Episode Num: 22 Episode T: 18 Reward: 18.000
Total T: 566 Episode Num: 23 Episode T: 20 Reward: 20.000
Total T: 596 Episode Num: 24 Episode T: 30 Reward: 30.000
Total T: 613 Episode Num: 25 Episode T: 17 Reward: 17.000
Total T: 627 Episode Num: 26 Episode T: 14 Reward: 14.000
Total T: 642 Episode Num: 27 Episode T: 15 Reward: 15.000
Total T: 670 Episode Num: 28 Episode T: 28 Reward: 28.000
Total T: 682 Episode Num: 29 Episode T: 12 Reward: 12.000
Total T: 693 Episode Num: 30 Episode T: 11 Reward: 11.000
Total T: 707 Episode Num: 31 Episode T: 14 Reward: 14.000
Total T: 727 Episode Num: 32 Episode T: 20 Reward: 20.000
Total T: 754 Episode Num: 33 Episode T: 27 Reward: 27.000
Total T: 784 Episode Num: 34 Episode T: 30 Reward: 30.000
Total T: 819 Episode Num: 35 Episode T: 35 Reward: 35.000
Total T: 849 Episode Num: 36 Episode T: 30 Reward: 30.000
Total T: 870 Episode Num: 37 Episode T: 21 Reward: 21.000
Total T: 885 Episode Num: 38 Episode T: 15 Reward: 15.000
Total T: 910 Episode Num: 39 Episode T: 25 Reward: 25.000
Total T: 925 Episode Num: 40 Episode T: 15 Reward: 15.000
Total T: 936 Episode Num: 41 Episode T: 11 Reward: 11.000
Total T: 949 Episode Num: 42 Episode T: 13 Reward: 13.000
Total T: 966 Episode Num: 43 Episode T: 17 Reward: 17.000
Total T: 987 Episode Num: 44 Episode T: 21 Reward: 21.000
Total T: 1001 Episode Num: 45 Episode T: 14 Reward: 14.000
Total T: 1011 Episode Num: 46 Episode T: 10 Reward: 10.000
Total T: 1021 Episode Num: 47 Episode T: 10 Reward: 10.000
Total T: 1029 Episode Num: 48 Episode T: 8 Reward: 8.000
Total T: 1056 Episode Num: 49 Episode T: 27 Reward: 27.000
Total T: 1065 Episode Num: 50 Episode T: 9 Reward: 9.000
Total T: 1075 Episode Num: 51 Episode T: 10 Reward: 10.000
Total T: 1085 Episode Num: 52 Episode T: 10 Reward: 10.000
Total T: 1096 Episode Num: 53 Episode T: 11 Reward: 11.000
Total T: 1104 Episode Num: 54 Episode T: 8 Reward: 8.000
Total T: 1112 Episode Num: 55 Episode T: 8 Reward: 8.000
Total T: 1121 Episode Num: 56 Episode T: 9 Reward: 9.000
Total T: 1131 Episode Num: 57 Episode T: 10 Reward: 10.000
Total T: 1142 Episode Num: 58 Episode T: 11 Reward: 11.000
Total T: 1154 Episode Num: 59 Episode T: 12 Reward: 12.000
Total T: 1166 Episode Num: 60 Episode T: 12 Reward: 12.000
Total T: 1176 Episode Num: 61 Episode T: 10 Reward: 10.000
Total T: 1185 Episode Num: 62 Episode T: 9 Reward: 9.000
Total T: 1195 Episode Num: 63 Episode T: 10 Reward: 10.000
Total T: 1205 Episode Num: 64 Episode T: 10 Reward: 10.000
Total T: 1216 Episode Num: 65 Episode T: 11 Reward: 11.000
Total T: 1225 Episode Num: 66 Episode T: 9 Reward: 9.000
Total T: 1234 Episode Num: 67 Episode T: 9 Reward: 9.000
Total T: 1244 Episode Num: 68 Episode T: 10 Reward: 10.000
Total T: 1252 Episode Num: 69 Episode T: 8 Reward: 8.000
Total T: 1262 Episode Num: 70 Episode T: 10 Reward: 10.000
Total T: 1272 Episode Num: 71 Episode T: 10 Reward: 10.000
Total T: 1282 Episode Num: 72 Episode T: 10 Reward: 10.000
Total T: 1291 Episode Num: 73 Episode T: 9 Reward: 9.000
Total T: 1300 Episode Num: 74 Episode T: 9 Reward: 9.000
Total T: 1310 Episode Num: 75 Episode T: 10 Reward: 10.000
Total T: 1320 Episode Num: 76 Episode T: 10 Reward: 10.000
Total T: 1330 Episode Num: 77 Episode T: 10 Reward: 10.000
Total T: 1343 Episode Num: 78 Episode T: 13 Reward: 13.000
Total T: 1352 Episode Num: 79 Episode T: 9 Reward: 9.000
Total T: 1361 Episode Num: 80 Episode T: 9 Reward: 9.000
Total T: 1371 Episode Num: 81 Episode T: 10 Reward: 10.000
Total T: 1380 Episode Num: 82 Episode T: 9 Reward: 9.000
Total T: 1390 Episode Num: 83 Episode T: 10 Reward: 10.000
Total T: 1403 Episode Num: 84 Episode T: 13 Reward: 13.000
Total T: 1416 Episode Num: 85 Episode T: 13 Reward: 13.000
Total T: 1426 Episode Num: 86 Episode T: 10 Reward: 10.000
Total T: 1438 Episode Num: 87 Episode T: 12 Reward: 12.000
Total T: 1449 Episode Num: 88 Episode T: 11 Reward: 11.000
Total T: 1457 Episode Num: 89 Episode T: 8 Reward: 8.000
Total T: 1468 Episode Num: 90 Episode T: 11 Reward: 11.000
Total T: 1478 Episode Num: 91 Episode T: 10 Reward: 10.000
Total T: 1498 Episode Num: 92 Episode T: 20 Reward: 20.000
Total T: 1508 Episode Num: 93 Episode T: 10 Reward: 10.000
Total T: 1519 Episode Num: 94 Episode T: 11 Reward: 11.000
Total T: 1528 Episode Num: 95 Episode T: 9 Reward: 9.000
Total T: 1549 Episode Num: 96 Episode T: 21 Reward: 21.000
Total T: 1560 Episode Num: 97 Episode T: 11 Reward: 11.000
Total T: 1569 Episode Num: 98 Episode T: 9 Reward: 9.000
Total T: 1590 Episode Num: 99 Episode T: 21 Reward: 21.000
Total T: 1642 Episode Num: 100 Episode T: 52 Reward: 52.000
Total T: 1692 Episode Num: 101 Episode T: 50 Reward: 50.000
Total T: 1828 Episode Num: 102 Episode T: 136 Reward: 136.000
Total T: 1977 Episode Num: 103 Episode T: 149 Reward: 149.000
Total T: 2112 Episode Num: 104 Episode T: 135 Reward: 135.000
Total T: 2259 Episode Num: 105 Episode T: 147 Reward: 147.000
Total T: 2400 Episode Num: 106 Episode T: 141 Reward: 141.000
Total T: 2588 Episode Num: 107 Episode T: 188 Reward: 188.000
Total T: 2761 Episode Num: 108 Episode T: 173 Reward: 173.000
Total T: 2961 Episode Num: 109 Episode T: 200 Reward: 200.000
Total T: 3112 Episode Num: 110 Episode T: 151 Reward: 151.000
Total T: 3305 Episode Num: 111 Episode T: 193 Reward: 193.000
Total T: 3505 Episode Num: 112 Episode T: 200 Reward: 200.000
Total T: 3674 Episode Num: 113 Episode T: 169 Reward: 169.000
Total T: 3837 Episode Num: 114 Episode T: 163 Reward: 163.000
Total T: 4005 Episode Num: 115 Episode T: 168 Reward: 168.000
Total T: 4161 Episode Num: 116 Episode T: 156 Reward: 156.000
Total T: 4361 Episode Num: 117 Episode T: 200 Reward: 200.000
Total T: 4561 Episode Num: 118 Episode T: 200 Reward: 200.000
Total T: 4735 Episode Num: 119 Episode T: 174 Reward: 174.000
Total T: 4919 Episode Num: 120 Episode T: 184 Reward: 184.000
---------------------------------------
Evaluation over 10 episodes: 195.400
---------------------------------------
Total T: 5119 Episode Num: 121 Episode T: 200 Reward: 200.000
Total T: 5292 Episode Num: 122 Episode T: 173 Reward: 173.000
Total T: 5454 Episode Num: 123 Episode T: 162 Reward: 162.000
Total T: 5606 Episode Num: 124 Episode T: 152 Reward: 152.000
Total T: 5806 Episode Num: 125 Episode T: 200 Reward: 200.000
Total T: 5980 Episode Num: 126 Episode T: 174 Reward: 174.000
Total T: 6155 Episode Num: 127 Episode T: 175 Reward: 175.000
Total T: 6351 Episode Num: 128 Episode T: 196 Reward: 196.000
Total T: 6537 Episode Num: 129 Episode T: 186 Reward: 186.000
Total T: 6707 Episode Num: 130 Episode T: 170 Reward: 170.000
Total T: 6849 Episode Num: 131 Episode T: 142 Reward: 142.000
Total T: 7014 Episode Num: 132 Episode T: 165 Reward: 165.000
Total T: 7189 Episode Num: 133 Episode T: 175 Reward: 175.000
Total T: 7382 Episode Num: 134 Episode T: 193 Reward: 193.000
Total T: 7554 Episode Num: 135 Episode T: 172 Reward: 172.000
Total T: 7754 Episode Num: 136 Episode T: 200 Reward: 200.000
Total T: 7922 Episode Num: 137 Episode T: 168 Reward: 168.000
Total T: 8092 Episode Num: 138 Episode T: 170 Reward: 170.000
Total T: 8292 Episode Num: 139 Episode T: 200 Reward: 200.000
Total T: 8459 Episode Num: 140 Episode T: 167 Reward: 167.000
Total T: 8650 Episode Num: 141 Episode T: 191 Reward: 191.000
Total T: 8724 Episode Num: 142 Episode T: 74 Reward: 74.000
Total T: 8924 Episode Num: 143 Episode T: 200 Reward: 200.000
Total T: 9074 Episode Num: 144 Episode T: 150 Reward: 150.000
Total T: 9264 Episode Num: 145 Episode T: 190 Reward: 190.000
Total T: 9430 Episode Num: 146 Episode T: 166 Reward: 166.000
Total T: 9630 Episode Num: 147 Episode T: 200 Reward: 200.000
Total T: 9820 Episode Num: 148 Episode T: 190 Reward: 190.000
---------------------------------------
Evaluation over 10 episodes: 196.700
---------------------------------------
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="generate-buffer">
<h2>Generate Buffer<a class="headerlink" href="#generate-buffer" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make env and determine properties</span>
<span class="n">env</span><span class="p">,</span> <span class="n">is_atari</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">num_actions</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_preprocessing</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_parameters</span> <span class="k">if</span> <span class="n">is_atari</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">regular_parameters</span>


<span class="c1"># Set seeds</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>


<span class="c1"># Initialize buffer</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">is_atari</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_preprocessing</span><span class="p">,</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">],</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;buffer_size&quot;</span><span class="p">],</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">setting</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">buffer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">buffer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">setting</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Initialize and load policy</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span>
    <span class="n">is_atari</span><span class="p">,</span>
    <span class="n">num_actions</span><span class="p">,</span>
    <span class="n">state_dim</span><span class="p">,</span>
    <span class="n">device</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;discount&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;optimizer&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;optimizer_parameters&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;polyak_target_update&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;target_update_freq&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;tau&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;initial_eps&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;end_eps&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eps_decay_period&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eval_eps&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">policy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./models/behavioral_</span><span class="si">{</span><span class="n">setting</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">evaluations</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="kc">False</span>
<span class="n">episode_start</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">episode_timesteps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">episode_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">low_noise_ep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">args</span><span class="o">.</span><span class="n">low_noise_p</span>
<span class="n">max_episode_steps</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="p">)</span><span class="o">.</span><span class="n">_max_episode_steps</span>

<span class="c1"># Interact with the environment for max_timesteps</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_timesteps</span><span class="p">)):</span>

    <span class="n">episode_timesteps</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># If generating the buffer, episode is low noise with p=low_noise_p.</span>
    <span class="c1"># If policy is low noise, we take random actions with p=eval_eps.</span>
    <span class="c1"># If the policy is high noise, we take random actions with p=rand_action_p.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">low_noise_ep</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">args</span><span class="o">.</span><span class="n">rand_action_p</span> <span class="o">-</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eval_eps&quot;</span><span class="p">]:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">state</span><span class="p">),</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Perform action and log results</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>

    <span class="c1"># Only consider &quot;done&quot; if episode terminates due to failure condition</span>
    <span class="n">done_float</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">done</span><span class="p">)</span> <span class="k">if</span> <span class="n">episode_timesteps</span> <span class="o">&lt;</span> <span class="n">max_episode_steps</span> <span class="k">else</span> <span class="mi">0</span>

    <span class="c1"># For atari, info[0] = clipped reward, info[1] = done_float</span>
    <span class="k">if</span> <span class="n">is_atari</span><span class="p">:</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">done_float</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
    <span class="c1"># Store data in replay buffer</span>
    <span class="n">replay_buffer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done_float</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">episode_start</span><span class="p">)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
    <span class="n">episode_start</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="c1"># +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total T: </span><span class="si">{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> Episode Num: </span><span class="si">{</span><span class="n">episode_num</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> Episode T: </span><span class="si">{</span><span class="n">episode_timesteps</span><span class="si">}</span><span class="s2"> Reward: </span><span class="si">{</span><span class="n">episode_reward</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># Reset environment</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(),</span> <span class="kc">False</span>
        <span class="n">episode_start</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">episode_timesteps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">episode_num</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">low_noise_ep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">args</span><span class="o">.</span><span class="n">low_noise_p</span>

<span class="c1"># Save final buffer and performance</span>
<span class="n">evaluations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eval_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./results/buffer_performance_</span><span class="si">{</span><span class="n">setting</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">evaluations</span><span class="p">)</span>
<span class="n">replay_buffer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./buffers/</span><span class="si">{</span><span class="n">buffer_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total T: 200 Episode Num: 1 Episode T: 200 Reward: 200.000
Total T: 400 Episode Num: 2 Episode T: 200 Reward: 200.000
Total T: 600 Episode Num: 3 Episode T: 200 Reward: 200.000
Total T: 721 Episode Num: 4 Episode T: 121 Reward: 121.000
Total T: 878 Episode Num: 5 Episode T: 157 Reward: 157.000
Total T: 1078 Episode Num: 6 Episode T: 200 Reward: 200.000
Total T: 1254 Episode Num: 7 Episode T: 176 Reward: 176.000
Total T: 1423 Episode Num: 8 Episode T: 169 Reward: 169.000
Total T: 1623 Episode Num: 9 Episode T: 200 Reward: 200.000
Total T: 1817 Episode Num: 10 Episode T: 194 Reward: 194.000
Total T: 2017 Episode Num: 11 Episode T: 200 Reward: 200.000
Total T: 2212 Episode Num: 12 Episode T: 195 Reward: 195.000
Total T: 2412 Episode Num: 13 Episode T: 200 Reward: 200.000
Total T: 2445 Episode Num: 14 Episode T: 33 Reward: 33.000
Total T: 2616 Episode Num: 15 Episode T: 171 Reward: 171.000
Total T: 2655 Episode Num: 16 Episode T: 39 Reward: 39.000
Total T: 2855 Episode Num: 17 Episode T: 200 Reward: 200.000
Total T: 2934 Episode Num: 18 Episode T: 79 Reward: 79.000
Total T: 3030 Episode Num: 19 Episode T: 96 Reward: 96.000
Total T: 3228 Episode Num: 20 Episode T: 198 Reward: 198.000
Total T: 3428 Episode Num: 21 Episode T: 200 Reward: 200.000
Total T: 3450 Episode Num: 22 Episode T: 22 Reward: 22.000
Total T: 3642 Episode Num: 23 Episode T: 192 Reward: 192.000
Total T: 3842 Episode Num: 24 Episode T: 200 Reward: 200.000
Total T: 3855 Episode Num: 25 Episode T: 13 Reward: 13.000
Total T: 4055 Episode Num: 26 Episode T: 200 Reward: 200.000
Total T: 4251 Episode Num: 27 Episode T: 196 Reward: 196.000
Total T: 4420 Episode Num: 28 Episode T: 169 Reward: 169.000
Total T: 4584 Episode Num: 29 Episode T: 164 Reward: 164.000
Total T: 4784 Episode Num: 30 Episode T: 200 Reward: 200.000
Total T: 4975 Episode Num: 31 Episode T: 191 Reward: 191.000
Total T: 5136 Episode Num: 32 Episode T: 161 Reward: 161.000
Total T: 5147 Episode Num: 33 Episode T: 11 Reward: 11.000
Total T: 5347 Episode Num: 34 Episode T: 200 Reward: 200.000
Total T: 5541 Episode Num: 35 Episode T: 194 Reward: 194.000
Total T: 5741 Episode Num: 36 Episode T: 200 Reward: 200.000
Total T: 5926 Episode Num: 37 Episode T: 185 Reward: 185.000
Total T: 6113 Episode Num: 38 Episode T: 187 Reward: 187.000
Total T: 6311 Episode Num: 39 Episode T: 198 Reward: 198.000
Total T: 6504 Episode Num: 40 Episode T: 193 Reward: 193.000
Total T: 6704 Episode Num: 41 Episode T: 200 Reward: 200.000
Total T: 6904 Episode Num: 42 Episode T: 200 Reward: 200.000
Total T: 7104 Episode Num: 43 Episode T: 200 Reward: 200.000
Total T: 7304 Episode Num: 44 Episode T: 200 Reward: 200.000
Total T: 7504 Episode Num: 45 Episode T: 200 Reward: 200.000
Total T: 7703 Episode Num: 46 Episode T: 199 Reward: 199.000
Total T: 7903 Episode Num: 47 Episode T: 200 Reward: 200.000
Total T: 8090 Episode Num: 48 Episode T: 187 Reward: 187.000
Total T: 8290 Episode Num: 49 Episode T: 200 Reward: 200.000
Total T: 8461 Episode Num: 50 Episode T: 171 Reward: 171.000
Total T: 8661 Episode Num: 51 Episode T: 200 Reward: 200.000
Total T: 8861 Episode Num: 52 Episode T: 200 Reward: 200.000
Total T: 9061 Episode Num: 53 Episode T: 200 Reward: 200.000
Total T: 9261 Episode Num: 54 Episode T: 200 Reward: 200.000
Total T: 9410 Episode Num: 55 Episode T: 149 Reward: 149.000
Total T: 9420 Episode Num: 56 Episode T: 10 Reward: 10.000
Total T: 9613 Episode Num: 57 Episode T: 193 Reward: 193.000
Total T: 9813 Episode Num: 58 Episode T: 200 Reward: 200.000
Total T: 9981 Episode Num: 59 Episode T: 168 Reward: 168.000
---------------------------------------
Evaluation over 10 episodes: 196.700
---------------------------------------
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="discrete-bcq">
<h2>Discrete BCQ<a class="headerlink" href="#discrete-bcq" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Used for Atari</span>
<span class="k">class</span> <span class="nc">Conv_Q</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frames</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">Conv_Q</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">c1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">c2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">c3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">q1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3136</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">q2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">i1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3136</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">i2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
		<span class="n">c</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
		<span class="n">c</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c2</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
		<span class="n">c</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c3</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>

		<span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q1</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3136</span><span class="p">)))</span>
		<span class="n">i</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">i1</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3136</span><span class="p">)))</span>
		<span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q2</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">i</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Used for Box2D / Toy problems</span>
<span class="k">class</span> <span class="nc">FC_Q</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">FC_Q</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">q1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">q2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">q3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">i1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">i2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">i3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>		


	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
		<span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
		<span class="n">q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q2</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>

		<span class="n">i</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">i1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
		<span class="n">i</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">i2</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
		<span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">i3</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q3</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">i</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">discrete_BCQ</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
		<span class="bp">self</span><span class="p">,</span> 
		<span class="n">is_atari</span><span class="p">,</span>
		<span class="n">num_actions</span><span class="p">,</span>
		<span class="n">state_dim</span><span class="p">,</span>
		<span class="n">device</span><span class="p">,</span>
		<span class="n">BCQ_threshold</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
		<span class="n">discount</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
		<span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span>
		<span class="n">optimizer_parameters</span><span class="o">=</span><span class="p">{},</span>
		<span class="n">polyak_target_update</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
		<span class="n">target_update_frequency</span><span class="o">=</span><span class="mf">8e3</span><span class="p">,</span>
		<span class="n">tau</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
		<span class="n">initial_eps</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
		<span class="n">end_eps</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
		<span class="n">eps_decay_period</span> <span class="o">=</span> <span class="mf">25e4</span><span class="p">,</span>
		<span class="n">eval_eps</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
	<span class="p">):</span>
	
		<span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

		<span class="c1"># Determine network type</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">Conv_Q</span><span class="p">(</span><span class="n">state_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_actions</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_atari</span> <span class="k">else</span> <span class="n">FC_Q</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_target</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_optimizer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_parameters</span><span class="p">)</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">discount</span> <span class="o">=</span> <span class="n">discount</span>

		<span class="c1"># Target update rule</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">maybe_update_target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">polyak_target_update</span> <span class="k">if</span> <span class="n">polyak_target_update</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy_target_update</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">target_update_frequency</span> <span class="o">=</span> <span class="n">target_update_frequency</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>

		<span class="c1"># Decay for eps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">initial_eps</span> <span class="o">=</span> <span class="n">initial_eps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">end_eps</span> <span class="o">=</span> <span class="n">end_eps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">slope</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">end_eps</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_eps</span><span class="p">)</span> <span class="o">/</span> <span class="n">eps_decay_period</span>

		<span class="c1"># Evaluation hyper-parameters</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">state_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">state_dim</span> <span class="k">if</span> <span class="n">is_atari</span> <span class="k">else</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">eval_eps</span> <span class="o">=</span> <span class="n">eval_eps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>

		<span class="c1"># Threshold for &quot;unlikely&quot; actions</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">BCQ_threshold</span>

		<span class="c1"># Number of training iterations</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">0</span>


	<span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="nb">eval</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
		<span class="c1"># Select action according to policy with probability (1-eps)</span>
		<span class="c1"># otherwise, select random action</span>
		<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_eps</span><span class="p">:</span>
			<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
				<span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
				<span class="n">q</span><span class="p">,</span> <span class="n">imt</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
				<span class="n">imt</span> <span class="o">=</span> <span class="n">imt</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
				<span class="n">imt</span> <span class="o">=</span> <span class="p">(</span><span class="n">imt</span><span class="o">/</span><span class="n">imt</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
				<span class="c1"># Use large negative number to mask actions from argmax</span>
				<span class="k">return</span> <span class="nb">int</span><span class="p">((</span><span class="n">imt</span> <span class="o">*</span> <span class="n">q</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">imt</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e8</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replay_buffer</span><span class="p">):</span>
		<span class="c1"># Sample replay buffer</span>
		<span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

		<span class="c1"># Compute the target Q value</span>
		<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
			<span class="n">q</span><span class="p">,</span> <span class="n">imt</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
			<span class="n">imt</span> <span class="o">=</span> <span class="n">imt</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
			<span class="n">imt</span> <span class="o">=</span> <span class="p">(</span><span class="n">imt</span><span class="o">/</span><span class="n">imt</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

			<span class="c1"># Use large negative number to mask actions from argmax</span>
			<span class="n">next_action</span> <span class="o">=</span> <span class="p">(</span><span class="n">imt</span> <span class="o">*</span> <span class="n">q</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">imt</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e8</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

			<span class="n">q</span><span class="p">,</span> <span class="n">imt</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_target</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
			<span class="n">target_Q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">done</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount</span> <span class="o">*</span> <span class="n">q</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">next_action</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

		<span class="c1"># Get current Q estimate</span>
		<span class="n">current_Q</span><span class="p">,</span> <span class="n">imt</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
		<span class="n">current_Q</span> <span class="o">=</span> <span class="n">current_Q</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>

		<span class="c1"># Compute Q loss</span>
		<span class="n">q_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="n">current_Q</span><span class="p">,</span> <span class="n">target_Q</span><span class="p">)</span>
		<span class="n">i_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">imt</span><span class="p">,</span> <span class="n">action</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

		<span class="n">Q_loss</span> <span class="o">=</span> <span class="n">q_loss</span> <span class="o">+</span> <span class="n">i_loss</span> <span class="o">+</span> <span class="mf">1e-2</span> <span class="o">*</span> <span class="n">i</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

		<span class="c1"># Optimize the Q</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
		<span class="n">Q_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

		<span class="c1"># Update target network by polyak or full copy every X iterations.</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">iterations</span> <span class="o">+=</span> <span class="mi">1</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">maybe_update_target</span><span class="p">()</span>


	<span class="k">def</span> <span class="nf">polyak_target_update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">target_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_target</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
		   <span class="n">target_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">copy_target_update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iterations</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_update_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
			 <span class="bp">self</span><span class="o">.</span><span class="n">Q_target</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make env and determine properties</span>
<span class="n">env</span><span class="p">,</span> <span class="n">is_atari</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">num_actions</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_preprocessing</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_parameters</span> <span class="k">if</span> <span class="n">is_atari</span> <span class="k">else</span> <span class="n">args</span><span class="o">.</span><span class="n">regular_parameters</span>


<span class="c1"># Set seeds</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>


<span class="c1"># Initialize buffer</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">is_atari</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">atari_preprocessing</span><span class="p">,</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">],</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;buffer_size&quot;</span><span class="p">],</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># For saving files</span>
<span class="n">setting</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">buffer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">buffer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">setting</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Initialize and load policy</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">discrete_BCQ</span><span class="p">(</span>
    <span class="n">is_atari</span><span class="p">,</span>
    <span class="n">num_actions</span><span class="p">,</span>
    <span class="n">state_dim</span><span class="p">,</span>
    <span class="n">device</span><span class="p">,</span>
    <span class="n">args</span><span class="o">.</span><span class="n">BCQ_threshold</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;discount&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;optimizer&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;optimizer_parameters&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;polyak_target_update&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;target_update_freq&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;tau&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;initial_eps&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;end_eps&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eps_decay_period&quot;</span><span class="p">],</span>
    <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eval_eps&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Load replay buffer	</span>
<span class="n">replay_buffer</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./buffers/</span><span class="si">{</span><span class="n">buffer_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">evaluations</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">episode_num</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">done</span> <span class="o">=</span> <span class="kc">True</span> 
<span class="n">training_iters</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">while</span> <span class="n">training_iters</span> <span class="o">&lt;</span> <span class="n">args</span><span class="o">.</span><span class="n">max_timesteps</span><span class="p">:</span> 
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eval_freq&quot;</span><span class="p">])):</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">)</span>

    <span class="n">evaluations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">eval_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">seed</span><span class="p">))</span>
    <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./results/BCQ_</span><span class="si">{</span><span class="n">setting</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">evaluations</span><span class="p">)</span>

    <span class="n">training_iters</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;eval_freq&quot;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training iterations: </span><span class="si">{</span><span class="n">training_iters</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Replay Buffer loaded with 10000 elements.
---------------------------------------
Evaluation over 10 episodes: 175.300
---------------------------------------
Training iterations: 5000
---------------------------------------
Evaluation over 10 episodes: 198.500
---------------------------------------
Training iterations: 10000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!apt-get -qq install tree
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!tree --du -h -C .
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Blue">.</span>
├── [864K]  <span class=" -Color -Color-Bold -Color-Bold-Blue">buffers</span>
│   ├── [ 78K]  Default_CartPole-v0_0_action.npy
│   ├── [313K]  Default_CartPole-v0_0_next_state.npy
│   ├── [ 78K]  Default_CartPole-v0_0_not_done.npy
│   ├── [ 136]  Default_CartPole-v0_0_ptr.npy
│   ├── [ 78K]  Default_CartPole-v0_0_reward.npy
│   └── [313K]  Default_CartPole-v0_0_state.npy
├── [801K]  <span class=" -Color -Color-Bold -Color-Bold-Blue">models</span>
│   ├── [531K]  behavioral_CartPole-v0_0_optimizer
│   └── [266K]  behavioral_CartPole-v0_0_Q
├── [4.4K]  <span class=" -Color -Color-Bold -Color-Bold-Blue">results</span>
│   ├── [ 144]  BCQ_CartPole-v0_0.npy
│   ├── [ 144]  behavioral_CartPole-v0_0.npy
│   └── [ 136]  buffer_performance_CartPole-v0_0.npy
├── [ 19M]  <span class=" -Color -Color-Bold -Color-Bold-Blue">ROM</span>
│   ├── [ 11M]  <span class=" -Color -Color-Bold -Color-Bold-Red">HC ROMS.zip</span>
│   └── [7.8M]  <span class=" -Color -Color-Bold -Color-Bold-Red">ROMS.zip</span>
├── [ 11M]  Roms.rar
├── [ 54M]  <span class=" -Color -Color-Bold -Color-Bold-Blue">sample_data</span>
│   ├── [1.7K]  <span class=" -Color -Color-Bold -Color-Bold-Green">anscombe.json</span>
│   ├── [294K]  california_housing_test.csv
│   ├── [1.6M]  california_housing_train.csv
│   ├── [ 17M]  mnist_test.csv
│   ├── [ 35M]  mnist_train_small.csv
│   └── [ 930]  <span class=" -Color -Color-Bold -Color-Bold-Green">README.md</span>
└── [ 65K]  <span class=" -Color -Color-Bold -Color-Bold-Blue">video</span>
    ├── [ 491]  openaigym.episode_batch.8.2302.stats.json
    ├── [ 406]  openaigym.manifest.8.2302.manifest.json
    ├── [2.0K]  openaigym.video.8.2302.video000000.meta.json
    ├── [ 18K]  openaigym.video.8.2302.video000000.mp4
    ├── [2.0K]  openaigym.video.8.2302.video000001.meta.json
    ├── [ 18K]  openaigym.video.8.2302.video000001.mp4
    ├── [2.0K]  openaigym.video.8.2302.video000008.meta.json
    └── [ 19K]  openaigym.video.8.2302.video000008.mp4

  86M used in 6 directories, 28 files
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">show_video</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><video alt="test" autoplay 
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,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" type="video/mp4" />
             </video></div></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sparsh-ai/drl-recsys",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T239645_Neural_Interactive_Collaborative_Filtering.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Neural Interactive Collaborative Filtering</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T616640_Pydeep_Recsys.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Pydeep Recsys</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>